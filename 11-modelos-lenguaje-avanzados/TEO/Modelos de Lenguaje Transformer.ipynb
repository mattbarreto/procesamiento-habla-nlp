{"cells":[{"cell_type":"markdown","metadata":{"id":"adFzzFsB-Ofl"},"source":["<h1>Explorando el Interior de Modelos de Lenguaje Transformer</h1>\n","<i>Un análisis profundo de la arquitectura transformer en modelos generativos de lenguaje</i>\n","\n","---\n","\n","Este cuaderno está adaptado del Capítulo 3 del libro [Hands-On Large Language Models](https://www.amazon.com/Hands-Large-Language-Models-Understanding/dp/1098150961)"]},{"cell_type":"markdown","metadata":{"id":"s9o0Yc3QNsY6"},"source":["### [OPCIONAL] - Instalación de Paquetes en <img src=\"https://colab.google/static/images/icons/colab.png\" width=100>\n","\n","Si estás ejecutando este cuaderno en Google Colab, necesitas **descomentar y ejecutar** el siguiente bloque de código para instalar las dependencias:\n","\n","---\n","\n","💡 **NOTA**: Para mejores resultados, usa una GPU. En Google Colab, ve a:\n","**Entorno de ejecución > Cambiar tipo de entorno de ejecución > Acelerador de hardware > GPU > Tipo de GPU > T4**.\n","\n","---"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"rDQT1WFoNsY7","executionInfo":{"status":"ok","timestamp":1748992001009,"user_tz":180,"elapsed":108908,"user":{"displayName":"BARRETO MATÍAS","userId":"15011433883683359534"}}},"outputs":[],"source":["%%capture\n","!pip install transformers>=4.41.2 accelerate>=0.31.0"]},{"cell_type":"markdown","metadata":{"id":"W_23Z_do-faF"},"source":["# Cargando el Modelo de Lenguaje"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-5RLd6dI-Ytm"},"outputs":[],"source":["from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n","\n","# Cargar modelo y tokenizador\n","tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    \"microsoft/Phi-3-mini-4k-instruct\",\n","    device_map=\"cuda\",\n","    torch_dtype=\"auto\",\n","    trust_remote_code=False,\n",")\n","\n","# Crear un pipeline de generación de texto\n","generator = pipeline(\n","    \"text-generation\",\n","    model=model,\n","    tokenizer=tokenizer,\n","    return_full_text=False,\n","    max_new_tokens=50,\n","    do_sample=False,\n",")"]},{"cell_type":"markdown","metadata":{"id":"REqcz-ID_XgV"},"source":["# Entradas y Salidas de un Modelo Transformer Entrenado"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"17h6TPHluJ-i"},"outputs":[],"source":["# Ejemplo adaptado al contexto argentino\n","prompt = \"Escribe un mensaje disculpándote con Juan por haber pisado su pie en el colectivo. Explica cómo sucedió.\"\n","\n","output = generator(prompt)\n","\n","print(output[0]['generated_text'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eoFkdTd6_g5o"},"outputs":[],"source":["# Mostrar la arquitectura del modelo\n","print(model)"]},{"cell_type":"markdown","metadata":{"id":"RTrwzB67BYVY"},"source":["# Seleccionando un token de la distribución de probabilidades (sampling/decoding)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sEcxYgJxBYbJ"},"outputs":[],"source":["prompt = \"La capital de Argentina es\"\n","\n","# Tokenizar el prompt de entrada\n","input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n","input_ids = input_ids.to(\"cuda\")\n","\n","# Obtener la salida del modelo antes de la capa lm_head\n","model_output = model.model(input_ids)\n","\n","# Obtener la salida de la capa lm_head\n","lm_head_output = model.lm_head(model_output[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"68YUSS4GBf9Q"},"outputs":[],"source":["# Obtener el token con mayor probabilidad\n","token_id = lm_head_output[0,-1].argmax(-1)\n","tokenizer.decode(token_id)"]},{"cell_type":"markdown","metadata":{"id":"Of2_rP4QBqrZ"},"source":["# Acelerando la generación con caché de keys y values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B0n6JhNHBrin"},"outputs":[],"source":["prompt = \"Escribe una carta muy larga explicando por qué el asado del domingo se quemó. Describe qué salió mal.\"\n","\n","# Tokenizar el prompt de entrada\n","input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n","input_ids = input_ids.to(\"cuda\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BwIvt6jSByAF"},"outputs":[],"source":["%%timeit -n 1\n","# Generar texto con caché activado\n","generation_output = model.generate(\n","  input_ids=input_ids,\n","  max_new_tokens=100,\n","  use_cache=True\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dFb1dcvJByCW"},"outputs":[],"source":["%%timeit -n 1\n","# Generar texto con caché desactivado\n","generation_output = model.generate(\n","  input_ids=input_ids,\n","  max_new_tokens=100,\n","  use_cache=False\n",")"]},{"cell_type":"markdown","metadata":{"id":"kwhrXCgpNsY-"},"source":["# Explicación\n","\n","## ¿Qué aprendimos?\n","\n","1. **Carga de modelos**: Cargamos el modelo Phi-3-mini, un modelo de lenguaje eficiente y potente.\n","2. **Generación de texto**: Vimos cómo el modelo puede generar texto coherente en español con ejemplos locales.\n","3. **Arquitectura interna**: Exploramos la estructura del modelo transformer.\n","4. **Proceso de decodificación**: Analizamos cómo el modelo selecciona la siguiente palabra.\n","5. **Optimización**: Comparamos el rendimiento con y sin caché de atención.\n","\n","## Aplicaciones\n","\n","Estos modelos pueden usarse para:\n","- Generar contenido educativo en español rioplatense\n","- Analizar textos legales y normativas locales\n","- Crear asistentes virtuales con conocimiento de cultura argentina\n","- Procesar documentos históricos y literarios nacionales"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.9"}},"nbformat":4,"nbformat_minor":0}