{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNMx3cBElwXsZLW0CnbMIxZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"2UYh3UNv5sqn"},"outputs":[],"source":["import requests\n","from bs4 import BeautifulSoup"]},{"cell_type":"markdown","source":["**requests:** Esta biblioteca se utiliza para hacer solicitudes HTTP a páginas web.\n","\n","**BeautifulSoup:** Esta biblioteca permite analizar (parsear) documentos HTML y XML, facilitando la extracción de información de ellos."],"metadata":{"id":"IhM8wf8DTL7K"}},{"cell_type":"markdown","source":["##Solicitud y análisis de una página web"],"metadata":{"id":"evsiOMoZTJOE"}},{"cell_type":"code","source":["contenido = requests.get(\"https://es.wikipedia.org/wiki/Categor%C3%ADa:Pel%C3%ADculas_ganadoras_del_premio_%C3%93scar_a_la_mejor_pel%C3%ADcula\").text\n","soup = BeautifulSoup(contenido, \"html.parser\")"],"metadata":{"id":"W1haN7Jt7hX1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**requests.get(...):** Realiza una solicitud GET a la URL proporcionada, que en este caso es la categoría de películas ganadoras del Óscar.  \n","**.text:** Obtiene el contenido de la respuesta en texto plano.  \n","**BeautifulSoup(contenido, \"html.parser\"):** Crea un objeto BeautifulSoup que permite manipular y extraer datos del HTML de la página."],"metadata":{"id":"TjmJKOaSTc4U"}},{"cell_type":"code","source":["#print(contenido)"],"metadata":{"id":"0CqVf2ko8D_Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#soup.find_all(\"a\")"],"metadata":{"id":"t6bS8jjr8YZ7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Extracción de enlaces"],"metadata":{"id":"1q_o6XFKTaJI"}},{"cell_type":"code","source":["for a in soup.find_all(\"a\", href=True):\n","  print(\"'https://en.wikipedia.org/\" + a[\"href\"] + \"' ,\")\n","  #print(a[\"href\"])"],"metadata":{"id":"9JBObvde8pQp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**soup.find_all(\"a\", href=True):** Busca todos los elementos `<a>` (enlaces) dentro del HTML que tengan un atributo `href`.\n","**print(...):** Imprime cada enlace encontrado. Los enlaces serán relativos, por lo que se concatenan con `https://en.wikipedia.org/` para formar una URL completa."],"metadata":{"id":"8Tgsd71HTqKK"}},{"cell_type":"markdown","source":["##Definición de una lista de URLs"],"metadata":{"id":"h58GGCCGTz0j"}},{"cell_type":"code","source":["lista = []"],"metadata":{"id":"XwW7w0BH-Ut-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["La lista contiene múltiples URLs, algunas de las cuales parecen repetidas o erróneas (por ejemplo, incluyen múltiples esquemas `https://` o enlaces no válidos).\n","Es importante limpiar y depurar esta lista para asegurar que se estén utilizando URLs válidas en el siguiente análisis."],"metadata":{"id":"dw1j6zDgT_CH"}},{"cell_type":"markdown","source":["##Extracción de textos de varios enlaces"],"metadata":{"id":"sdOyEPUjUB14"}},{"cell_type":"code","source":["text = \" \"\n","\n","for item in lista:\n","  texto_html = requests.get(item).text\n","  soup = BeautifulSoup(texto_html, \"html.parser\")\n","  for paragraphe in soup.find_all(\"p\"):\n","    text += str(paragraphe.getText())\n","\n","print(text)"],"metadata":{"id":"WggNfUux-oYj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Extracción de textos de varios enlaces**\n","\n","Se inicializa una variable `text` como cadena vacía.\n","Se itera sobre cada elemento de la lista para realizar solicitudes HTTP.\n","Se utiliza BeautifulSoup para parsear cada página y extraer texto de los párrafos (`<p>`).\n","Se concatenan todos los textos extraídos en la variable `text`."],"metadata":{"id":"41wC1X_LUMto"}},{"cell_type":"markdown","source":["##Conteo de palabras"],"metadata":{"id":"ByCBJAK8UNqx"}},{"cell_type":"code","source":["from collections import Counter\n","import re"],"metadata":{"id":"gvNDSeq0AGyw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lista_de_palabras = re.sub(\"[^\\w]\", \" \", text).split()\n","contador = Counter(lista_de_palabras)\n","\n","print(contador)"],"metadata":{"id":"a-2Kw4RGAJPp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**re.sub(\"[^\\w]\", \" \", text):** Reemplaza todos los caracteres que no son letras o números en `text` con espacios.\n","**.split():** Divide el texto en palabras, creando una lista de palabras.\n","**Counter(lista_de_palabras):** Crea un contador que calcula la frecuencia de cada palabra en la lista.\n","**print(contador):** Muestra la cantidad de veces que aparece cada palabra."],"metadata":{"id":"kYv_2WicUZYc"}},{"cell_type":"markdown","source":["## Conclusiones y Recomendaciones\n","\n","Este cuaderno realiza un scraping básico de una categoría de Wikipedia, ideal para aprender conceptos de extracción de datos y manipulación de texto.\n","\n","### Recomendaciones para mejorar el código:\n","\n","* **Profundizar en técnicas de limpieza de datos para mejorar la calidad del análisis final:** Explorar bibliotecas como `NLTK` o `spaCy` para realizar un preprocesamiento más avanzado del texto, incluyendo la eliminación de stop words, stemming o lematización."],"metadata":{"id":"ZnDJaBIrUV03"}}]}