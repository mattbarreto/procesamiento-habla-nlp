{"cells":[{"cell_type":"markdown","metadata":{"id":"11uLD9O6q0de"},"source":["# Aplicaciones de Transformers en Procesamiento de Lenguaje Natural\n","\n","Este cuaderno explora distintas aplicaciones de los modelos Transformers en tareas de procesamiento de lenguaje natural (PLN), utilizando ejemplos pr치cticos y explicaciones te칩ricas.\n","\n","---\n","\n","## 쯈u칠 es la arquitectura Transformer?\n","\n","La **arquitectura Transformer** es un tipo de red neuronal propuesta por Vaswani et al. en 2017. Su principal innovaci칩n es el mecanismo de *atenci칩n*, que permite al modelo enfocarse en diferentes partes de la entrada para procesar informaci칩n contextual de manera eficiente. Los Transformers reemplazaron a modelos anteriores como LSTM y GRU en muchas tareas de PLN debido a su capacidad para manejar secuencias largas y aprender dependencias complejas.\n","\n","---\n","\n","## 쯈u칠 es la librer칤a Transformers de Hugging Face?\n","\n","La **librer칤a Transformers de Hugging Face** es una biblioteca de Python que proporciona implementaciones preentrenadas de modelos basados en la arquitectura Transformer (como BERT, GPT, T5, etc.) para tareas de PLN. Permite cargar modelos f치cilmente y utilizarlos en tareas como clasificaci칩n, traducci칩n, resumen, etc., sin necesidad de entrenarlos desde cero. Podes explorar los modelos disponibles en la [p치gina de modelos de Hugging Face](https://huggingface.co/models).\n","\n","---\n","\n","## 쯈u칠 es un pipeline en Hugging Face?\n","\n","Un **pipeline** en Hugging Face es una interfaz de alto nivel que simplifica el uso de modelos preentrenados para tareas espec칤ficas. Permite ejecutar tareas como clasificaci칩n de texto, reconocimiento de entidades, traducci칩n, etc., con solo unas pocas l칤neas de c칩digo, ocultando detalles complejos de preprocesamiento y postprocesamiento.\n"],"id":"11uLD9O6q0de"},{"cell_type":"markdown","metadata":{"id":"pGi_AA0Cq0dh"},"source":["## 1. Definir el texto de ejemplo\n","\n","A continuaci칩n, se presenta un texto de ejemplo que ser치 utilizado en las distintas aplicaciones de modelos Transformers. Este texto simula una queja de un cliente sobre un pedido equivocado, lo que nos permitir치 explorar tareas como clasificaci칩n, reconocimiento de entidades, respuesta a preguntas, resumen, traducci칩n y generaci칩n de texto."],"id":"pGi_AA0Cq0dh"},{"cell_type":"code","metadata":{"id":"c5qDBq2Wq0di"},"source":["texto = \"\"\"Querido MercadoLibre, la semana pasada ped칤 una figura de acci칩n de Optimus Prime desde su tienda online.\n","Para mi sorpresa, cuando abr칤 el paquete, descubr칤 horrorizado que me hab칤an enviado una figura de Megatron.\n","Como fan de los Autobots, espero que entiendan mi decepci칩n. Solicito un cambio urgente del producto.\"\"\""],"id":"c5qDBq2Wq0di","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a12gU1Xeq0di"},"source":["## 2. Clasificaci칩n de texto con Transformers\n","\n","La **clasificaci칩n de texto** es una tarea fundamental en PLN que consiste en asignar una o varias etiquetas a un texto, como por ejemplo identificar el sentimiento (positivo, negativo, neutro) o la intenci칩n del mensaje. Los modelos Transformers, gracias a su capacidad para comprender el contexto, demostraron un rendimiento sobresaliente en esta tarea.\n","\n","Utilizaremos el pipeline `text-classification` de la librer칤a 游뱅 Transformers para analizar el texto de ejemplo y visualizar el resultado de la clasificaci칩n."],"id":"a12gU1Xeq0di"},{"cell_type":"code","metadata":{"id":"fbb6MppZq0dj"},"source":["from transformers import pipeline\n","# Modelo de clasificaci칩n de texto (sentimiento) en espa침ol\n","# Puedes buscar otros modelos en https://huggingface.co/models\n","classifier = pipeline(\"text-classification\", model=\"pysentimiento/robertuito-sentiment-analysis\")\n","import pandas as pd\n","outputs = classifier(texto)\n","pd.DataFrame(outputs)"],"id":"fbb6MppZq0dj","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZD-puqTdq0dj"},"source":["## 3. Reconocimiento de entidades nombradas (NER)\n","\n","El **Reconocimiento de Entidades Nombradas** (NER, por sus siglas en ingl칠s) es la tarea de identificar y clasificar autom치ticamente nombres propios, lugares, organizaciones y otras entidades relevantes dentro de un texto. Los Transformers permiten realizar NER de manera eficiente y precisa.\n","\n","A continuaci칩n, aplicamos el pipeline `ner` al texto de ejemplo para extraer las entidades nombradas presentes."],"id":"ZD-puqTdq0dj"},{"cell_type":"code","metadata":{"id":"nyeoJh8Kq0dj"},"source":["# Modelo de reconocimiento de entidades nombradas (NER) en espa침ol\n","# Puedes buscar otros modelos en https://huggingface.co/models\n","ner_tagger = pipeline(\"ner\", model=\"mrm8488/bert-spanish-cased-finetuned-ner\", aggregation_strategy=\"simple\")\n","outputs = ner_tagger(texto)\n","pd.DataFrame(outputs)"],"id":"nyeoJh8Kq0dj","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wngEBUxYq0dj"},"source":["## 4. Respuesta a preguntas basada en contexto\n","\n","La tarea de **respuesta a preguntas** (Question Answering) consiste en responder preguntas espec칤ficas utilizando un contexto dado. Los modelos Transformers pueden comprender el texto y extraer la informaci칩n relevante para responder de manera precisa.\n","\n","En el siguiente ejemplo, preguntamos al modelo qu칠 desea el cliente, utilizando el pipeline `question-answering`."],"id":"wngEBUxYq0dj"},{"cell_type":"code","metadata":{"id":"sfCRZYcxq0dk"},"source":["# Modelo de respuesta a preguntas en espa침ol\n","# Puedes buscar otros modelos en https://huggingface.co/models\n","reader = pipeline(\"question-answering\", model=\"PlanTL-GOB-ES/roberta-large-bne-sqac\")\n","question = \"쯈u칠 quiere el cliente?\"\n","outputs = reader(question=question, context=texto)\n","pd.DataFrame([outputs])"],"id":"sfCRZYcxq0dk","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O2imwRuwq0dk"},"source":["## 5. Resumen autom치tico de texto\n","\n","El **resumen autom치tico** permite condensar la informaci칩n principal de un texto extenso en una versi칩n m치s corta, manteniendo el significado esencial. Los Transformers han revolucionado esta tarea gracias a su capacidad de comprensi칩n contextual.\n","\n","A continuaci칩n, utilizamos el pipeline `summarization` para obtener un resumen del texto de ejemplo."],"id":"O2imwRuwq0dk"},{"cell_type":"code","metadata":{"id":"5f6KQq23q0dk"},"source":["from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n","# Puedes buscar otros modelos en https://huggingface.co/models\n","model_name = \"csebuetnlp/mT5_multilingual_XLSum\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n","resumidor = pipeline(\"summarization\", model=model, tokenizer=tokenizer)\n","resumen = resumidor(texto, max_length=80, min_length=20, do_sample=False)\n","print(resumen[0]['summary_text'])"],"id":"5f6KQq23q0dk","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"knv2k1Bbq0dk"},"source":["## 6. Traducci칩n autom치tica del espa침ol a ingl칠s\n","\n","La **traducci칩n autom치tica** es una de las aplicaciones m치s destacadas de los Transformers, permitiendo traducir textos entre diferentes idiomas con alta calidad. Utilizaremos el pipeline `translation_es_to_en` para traducir el texto de ejemplo del espa침ol al ingl칠s."],"id":"knv2k1Bbq0dk"},{"cell_type":"code","metadata":{"id":"P_8xjN6vq0dk"},"source":["# Modelo de traducci칩n de espa침ol a ingl칠s\n","# Puedes buscar otros modelos en https://huggingface.co/models\n","translator = pipeline(\"translation_es_to_en\", model=\"Helsinki-NLP/opus-mt-es-en\")\n","outputs = translator(texto)\n","print(outputs[0]['translation_text'])"],"id":"P_8xjN6vq0dk","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Vdr-gK_aq0dl"},"source":["## 7. Generaci칩n autom치tica de texto (respuesta de servicio al cliente)\n","\n","La **generaci칩n autom치tica de texto** permite crear o continuar textos de manera coherente a partir de un prompt inicial. Esta capacidad es 칰til, por ejemplo, para redactar respuestas autom치ticas en atenci칩n al cliente.\n","\n","En el siguiente ejemplo, generamos una posible respuesta de servicio al cliente utilizando el pipeline `text-generation`."],"id":"Vdr-gK_aq0dl"},{"cell_type":"code","metadata":{"id":"1LuWAKRMq0dl"},"source":["# Modelo de generaci칩n de texto en espa침ol\n","# Puedes buscar otros modelos en https://huggingface.co/models\n","generator = pipeline(\"text-generation\", model=\"datificate/gpt2-small-spanish\")\n","respuesta_inicial = \"Estimado cliente, lamentamos mucho lo ocurrido con su pedido. \"\n","prompt = texto + \"\\n\\nRespuesta del servicio al cliente:\\n\" + respuesta_inicial\n","outputs = generator(\n","    prompt,\n","    max_new_tokens=150,\n","    do_sample=True,\n","    temperature=0.7,\n","    top_k=50,\n","    top_p=0.9,\n","    repetition_penalty=1.3,\n","    eos_token_id=50256\n",")\n","print(outputs[0]['generated_text'])"],"id":"1LuWAKRMq0dl","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fwOnmW33q0dl"},"source":["---\n","\n","## Ejercicios aut칩nomos (40 minutos)\n","\n","A continuaci칩n, te proponemos ejercicios pr치cticos para que pongas en pr치ctica lo aprendido sobre la arquitectura Transformer, la librer칤a Transformers de Hugging Face y el uso de pipelines en tareas de PLN.\n","\n","**Recorda:** Para cada ejercicio, explora la [p치gina de modelos de Hugging Face](https://huggingface.co/models) y busca modelos que sean apropiados para espa침ol, o incluso para el contexto argentino si los encontras.\n","\n","### 1. Clasificaci칩n de textos propios\n","- Escribi dos textos breves (pueden ser opiniones, quejas o comentarios sobre servicios o productos en Argentina, por ejemplo sobre colectivos, supermercados, bancos, etc.).\n","- Busca en Hugging Face un modelo de clasificaci칩n de texto en espa침ol y usalo con el pipeline `text-classification` para analizar el sentimiento de cada texto.\n","- 쮼l resultado coincide con tu expectativa? Explica brevemente.\n","\n","### 2. Reconocimiento de entidades en noticias argentinas\n","- Busca un p치rrafo de una noticia reciente de un medio argentino (por ejemplo, Clar칤n, La Naci칩n, P치gina/12).\n","- Busca en Hugging Face un modelo de NER para espa침ol y apl칤calo para identificar entidades nombradas.\n","- Enumera las entidades encontradas y clasif칤calas (persona, organizaci칩n, lugar, etc.).\n","\n","### 3. Respuesta a preguntas personalizada\n","- Escribi un peque침o texto narrativo (4-5 l칤neas) sobre una situaci칩n cotidiana en Buenos Aires (por ejemplo, un reclamo en una oficina p칰blica, una experiencia en el subte, etc.).\n","- Formula dos preguntas sobre el texto y utiliza un modelo de `question-answering` de Hugging Face para responderlas.\n","- 쯃as respuestas son correctas y precisas? 쯇or qu칠?\n","\n","### 4. Resumen de un texto propio\n","- Eleg칤 un texto m치s largo (puede ser un fragmento de Wikipedia sobre Argentina, una noticia, etc.).\n","- Busca un modelo de resumen en espa침ol en Hugging Face y 칰salo con el pipeline `summarization`.\n","- Compara el resumen generado con el texto original: 쯤u칠 informaci칩n se perdi칩 y cu치l se mantuvo?\n","\n","### 5. Traducci칩n y an치lisis\n","- Escribi un texto breve en espa침ol rioplatense y trad칰cilo al ingl칠s usando un modelo de traducci칩n de Hugging Face.\n","- Luego, traduci el resultado nuevamente al espa침ol (podes usar otro pipeline o Google Translate).\n","- 쮼l texto final es igual al original? 쯈u칠 diferencias encontras?\n","\n","### 6. Generaci칩n creativa de texto\n","- Escribe el inicio de una historia o una pregunta abierta relacionada con Buenos Aires.\n","- Busca un modelo de generaci칩n de texto en espa침ol en Hugging Face y 칰salo con el pipeline `text-generation` para continuar el texto.\n","- Analiza la coherencia y creatividad de la respuesta generada.\n","\n","---\n","\n","**Tip:** Para cada ejercicio, explora la [p치gina de modelos de Hugging Face](https://huggingface.co/models) y lee la descripci칩n de los modelos antes de usarlos. Si tienes dudas, consulta la documentaci칩n oficial de [Transformers](https://huggingface.co/docs/transformers/index)."],"id":"fwOnmW33q0dl"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":""},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}