# Módulo 7: Word Embeddings - Teoría Avanzada

## 🎯 Objetivos del Módulo

- Profundizar en los fundamentos matemáticos de word embeddings
- Analizar la evolución histórica de las representaciones vectoriales
- Evaluar la calidad y sesgos en embeddings
- Preparar la transición hacia representaciones contextuales

## 📚 Contenido

### Presentación Principal
- `Word_Embeddings_y_Word2Vec.pptx`: Fundamentos teóricos completos

## 📊 Temas Teóricos Cubiertos

### Fundamentos Matemáticos
- Espacios vectoriales y métricas de similitud
- Reducción de dimensionalidad
- Optimización de funciones objetivo
- Algebra lineal aplicada a NLP

### Evolución Histórica
- De one-hot encoding a representaciones densas
- Limitaciones de métodos tradicionales
- Breakthrough de Word2Vec
- Comparación con métodos posteriores

### Evaluación de Embeddings
- Métricas de similitud semántica
- Benchmarks estándar
- Evaluación intrínseca vs extrínseca
- Detección de sesgos

## 🔍 Análisis Crítico

### Ventajas de Word Embeddings
- Captura de relaciones semánticas
- Eficiencia computacional
- Transferibilidad entre tareas
- Interpretabilidad relativa

### Limitaciones Identificadas
- Polisemia no resuelta
- Sesgos sociales y culturales
- Dependencia del corpus de entrenamiento
- Falta de contexto dinámico

## 🎓 Competencias Desarrolladas

- ✅ Comprensión teórica profunda
- ✅ Análisis crítico de métodos
- ✅ Evaluación de calidad de embeddings
- ✅ Identificación de limitaciones y sesgos

## 🎯 Metodología

- **Brainstorming**: "¿Cómo podemos evaluar la calidad de los word embeddings y detectar posibles sesgos?"
- **Análisis Teórico**: Discusión de papers fundamentales
- **Reflexión Crítica**: Limitaciones y direcciones futuras

## 🌟 Preparación para Módulos Avanzados

Este módulo sienta las bases teóricas para comprender:
- Limitaciones que resuelven los Transformers
- Necesidad de representaciones contextuales
- Evolución hacia modelos de lenguaje modernos

---
*Prof. Matias Barreto - Laboratorio de Introducción al NLP y LLMs*
