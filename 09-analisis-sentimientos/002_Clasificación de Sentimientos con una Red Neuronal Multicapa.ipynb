{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNp5mG8hgZvyQpu8yU5GFtc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Clasificación de Sentimientos con una Red Neuronal Multicapa (PyTorch)\n","##🎯 Objetivo\n","En esta actividad vas a construir una red neuronal feedforward multicapa (MLP) usando PyTorch. El objetivo es entrenarla para que pueda clasificar frases en español como positivas o negativas.\n","\n","###Con esto vas a:\n","\n","* Comprender cómo se arma una red con varias neuronas.\n","\n","* Usar funciones de activación y entrenamiento automático.\n","\n","* Observar cómo mejora respecto al perceptrón simple de la Actividad 1."],"metadata":{"id":"xPAukcLZZOqz"}},{"cell_type":"markdown","source":["##🧰 1. Preparación del entorno\n","Importamos PyTorch y NumPy para comenzar."],"metadata":{"id":"bp3C44_ubQAZ"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np"],"metadata":{"id":"XUvckjUsa0TD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##🗂️ 2. Datos de entrenamiento\n","Usamos un conjunto de frases típicas de opiniones escritas en Argentina, etiquetadas como positivas (1) o negativas (0)."],"metadata":{"id":"iuJrPnpWbUKJ"}},{"cell_type":"code","source":["frases = [\n","    \"La verdad, este lugar está bárbaro. Muy recomendable\",\n","    \"Una porquería de servicio, nunca más vuelvo\",\n","    \"Me encantó la comida, aunque la música estaba muy fuerte\",\n","    \"El envío fue lento y el producto llegó dañado. Qué desastre\",\n","    \"Todo excelente. Atención de diez\",\n","    \"Qué estafa, me arrepiento de haber comprado\",\n","    \"Muy conforme con el resultado final\",\n","    \"No me gustó para nada la experiencia\",\n","    \"Superó mis expectativas, ¡gracias!\",\n","    \"No lo recomiendo, mala calidad\"\n","]\n","\n","etiquetas = np.array([1, 0, 1, 0, 1, 0, 1, 0, 1, 0])  # 1 = Positivo, 0 = Negativo\n"],"metadata":{"id":"HFBeK5ZGbYF5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##🧾 3. Construcción del vocabulario\n","Definimos manualmente un vocabulario con palabras que suelen aparecer en frases de opinión con carga positiva o negativa."],"metadata":{"id":"fZAj_cxtba9R"}},{"cell_type":"code","source":["vocabulario = [\n","    \"bárbaro\", \"recomendable\", \"porquería\", \"nunca\", \"encantó\",\n","    \"fuerte\", \"desastre\", \"excelente\", \"estafa\", \"arrepiento\",\n","    \"conforme\", \"gustó\", \"superó\", \"gracias\", \"recomiendo\", \"mala\"\n","]"],"metadata":{"id":"1KEXTHNUbalR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","##🧠 4. Preprocesamiento: vectorización de las frases\n","Cada frase se convierte en un vector binario (bag-of-words) que indica si contiene alguna de las palabras del vocabulario."],"metadata":{"id":"OP-pT2DTbk5K"}},{"cell_type":"code","source":["def vectorizar(frase, vocabulario):\n","    tokens = frase.lower().split()\n","    return np.array([1 if palabra in tokens else 0 for palabra in vocabulario], dtype=np.float32)\n","\n","X_np = np.array([vectorizar(frase, vocabulario) for frase in frases], dtype=np.float32)\n","y_np = etiquetas.astype(np.float32).reshape(-1, 1)\n","\n","# Convertimos a tensores de PyTorch\n","X = torch.tensor(X_np)\n","y = torch.tensor(y_np)"],"metadata":{"id":"VaCF540vbnoR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##🧱 5. Definición del modelo (MLP)\n","Vamos a crear un modelo simple con una capa oculta, activación ReLU, y una salida sigmoide para predicción binaria."],"metadata":{"id":"3rQdH7w3bsXB"}},{"cell_type":"code","source":["input_size = len(vocabulario)\n","hidden_size = 8\n","\n","class MLP(nn.Module):\n","    def __init__(self):\n","        super(MLP, self).__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(input_size, hidden_size),\n","            nn.ReLU(),\n","            nn.Linear(hidden_size, 1),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, x):\n","        return self.net(x)\n","\n","modelo = MLP()"],"metadata":{"id":"sL_K2o15bvQZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##⚙️ 6. Entrenamiento del modelo\n","Definimos la función de pérdida y el optimizador. Entrenamos por varias épocas."],"metadata":{"id":"h4-EQNcib3tB"}},{"cell_type":"code","source":["criterio = nn.BCELoss()  # Binary Cross Entropy\n","optimizador = optim.Adam(modelo.parameters(), lr=0.01)\n","\n","epocas = 200\n","\n","for epoca in range(epocas):\n","    modelo.train()\n","    salida = modelo(X)\n","    loss = criterio(salida, y)\n","\n","    optimizador.zero_grad()\n","    loss.backward()\n","    optimizador.step()\n","\n","    if (epoca + 1) % 10 == 0:\n","        print(f\"Época {epoca+1}, Pérdida: {loss.item():.4f}\")\n"],"metadata":{"id":"2epH-XRHb58g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##🧪 7. Evaluación con frases nuevas\n","Probamos la red con frases que no estaban en el entrenamiento, para ver cómo generaliza."],"metadata":{"id":"YKZ5V6yhcDxI"}},{"cell_type":"code","source":["frases_prueba = [\n","    \"No me gustó la atención, bastante mala\",\n","    \"Muy buena experiencia, todo excelente\",\n","    \"Una estafa total, no lo recomiendo\",\n","    \"Súper conforme con el servicio\",\n","    \"Nada que ver con lo prometido, una decepción\"\n","]\n","\n","# Vectorizamos las frases de prueba\n","X_prueba_np = np.array([vectorizar(frase, vocabulario) for frase in frases_prueba], dtype=np.float32)\n","X_prueba = torch.tensor(X_prueba_np)\n","\n","# Predicción\n","modelo.eval()\n","with torch.no_grad():\n","    predicciones = modelo(X_prueba)\n","\n","# Mostrar resultados\n","for frase, pred in zip(frases_prueba, predicciones):\n","    clase = \"Positivo\" if pred.item() >= 0.5 else \"Negativo\"\n","    print(f\"Frase: '{frase}' => Sentimiento predicho: {clase} ({pred.item():.2f})\")"],"metadata":{"id":"gpIGfi-tcFPY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##💬 Reflexión final\n","###👉 ¿Qué aprendimos?\n","\n","* Cómo implementar y entrenar una red neuronal multicapa para análisis de sentimiento.\n","\n","* Cómo preprocesar texto en español usando bag-of-words.\n","\n","* Las ventajas del MLP frente al perceptrón simple.\n","\n","* Limitaciones: aún no capta el orden de las palabras ni el contexto secuencial.\n","\n","➡️ En la próxima actividad aprenderemos a usar redes recurrentes (LSTM) para incorporar secuencia y memoria en el procesamiento de texto. ¡Nos acercamos a modelos más cercanos al lenguaje humano!"],"metadata":{"id":"138SwAUvcRnQ"}}]}