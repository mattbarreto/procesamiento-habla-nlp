{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNp5mG8hgZvyQpu8yU5GFtc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#ClasificaciÃ³n de Sentimientos con una Red Neuronal Multicapa (PyTorch)\n","##ğŸ¯ Objetivo\n","En esta actividad vas a construir una red neuronal feedforward multicapa (MLP) usando PyTorch. El objetivo es entrenarla para que pueda clasificar frases en espaÃ±ol como positivas o negativas.\n","\n","###Con esto vas a:\n","\n","* Comprender cÃ³mo se arma una red con varias neuronas.\n","\n","* Usar funciones de activaciÃ³n y entrenamiento automÃ¡tico.\n","\n","* Observar cÃ³mo mejora respecto al perceptrÃ³n simple de la Actividad 1."],"metadata":{"id":"xPAukcLZZOqz"}},{"cell_type":"markdown","source":["##ğŸ§° 1. PreparaciÃ³n del entorno\n","Importamos PyTorch y NumPy para comenzar."],"metadata":{"id":"bp3C44_ubQAZ"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np"],"metadata":{"id":"XUvckjUsa0TD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##ğŸ—‚ï¸ 2. Datos de entrenamiento\n","Usamos un conjunto de frases tÃ­picas de opiniones escritas en Argentina, etiquetadas como positivas (1) o negativas (0)."],"metadata":{"id":"iuJrPnpWbUKJ"}},{"cell_type":"code","source":["frases = [\n","    \"La verdad, este lugar estÃ¡ bÃ¡rbaro. Muy recomendable\",\n","    \"Una porquerÃ­a de servicio, nunca mÃ¡s vuelvo\",\n","    \"Me encantÃ³ la comida, aunque la mÃºsica estaba muy fuerte\",\n","    \"El envÃ­o fue lento y el producto llegÃ³ daÃ±ado. QuÃ© desastre\",\n","    \"Todo excelente. AtenciÃ³n de diez\",\n","    \"QuÃ© estafa, me arrepiento de haber comprado\",\n","    \"Muy conforme con el resultado final\",\n","    \"No me gustÃ³ para nada la experiencia\",\n","    \"SuperÃ³ mis expectativas, Â¡gracias!\",\n","    \"No lo recomiendo, mala calidad\"\n","]\n","\n","etiquetas = np.array([1, 0, 1, 0, 1, 0, 1, 0, 1, 0])  # 1 = Positivo, 0 = Negativo\n"],"metadata":{"id":"HFBeK5ZGbYF5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##ğŸ§¾ 3. ConstrucciÃ³n del vocabulario\n","Definimos manualmente un vocabulario con palabras que suelen aparecer en frases de opiniÃ³n con carga positiva o negativa."],"metadata":{"id":"fZAj_cxtba9R"}},{"cell_type":"code","source":["vocabulario = [\n","    \"bÃ¡rbaro\", \"recomendable\", \"porquerÃ­a\", \"nunca\", \"encantÃ³\",\n","    \"fuerte\", \"desastre\", \"excelente\", \"estafa\", \"arrepiento\",\n","    \"conforme\", \"gustÃ³\", \"superÃ³\", \"gracias\", \"recomiendo\", \"mala\"\n","]"],"metadata":{"id":"1KEXTHNUbalR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","##ğŸ§  4. Preprocesamiento: vectorizaciÃ³n de las frases\n","Cada frase se convierte en un vector binario (bag-of-words) que indica si contiene alguna de las palabras del vocabulario."],"metadata":{"id":"OP-pT2DTbk5K"}},{"cell_type":"code","source":["def vectorizar(frase, vocabulario):\n","    tokens = frase.lower().split()\n","    return np.array([1 if palabra in tokens else 0 for palabra in vocabulario], dtype=np.float32)\n","\n","X_np = np.array([vectorizar(frase, vocabulario) for frase in frases], dtype=np.float32)\n","y_np = etiquetas.astype(np.float32).reshape(-1, 1)\n","\n","# Convertimos a tensores de PyTorch\n","X = torch.tensor(X_np)\n","y = torch.tensor(y_np)"],"metadata":{"id":"VaCF540vbnoR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##ğŸ§± 5. DefiniciÃ³n del modelo (MLP)\n","Vamos a crear un modelo simple con una capa oculta, activaciÃ³n ReLU, y una salida sigmoide para predicciÃ³n binaria."],"metadata":{"id":"3rQdH7w3bsXB"}},{"cell_type":"code","source":["input_size = len(vocabulario)\n","hidden_size = 8\n","\n","class MLP(nn.Module):\n","    def __init__(self):\n","        super(MLP, self).__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(input_size, hidden_size),\n","            nn.ReLU(),\n","            nn.Linear(hidden_size, 1),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, x):\n","        return self.net(x)\n","\n","modelo = MLP()"],"metadata":{"id":"sL_K2o15bvQZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##âš™ï¸ 6. Entrenamiento del modelo\n","Definimos la funciÃ³n de pÃ©rdida y el optimizador. Entrenamos por varias Ã©pocas."],"metadata":{"id":"h4-EQNcib3tB"}},{"cell_type":"code","source":["criterio = nn.BCELoss()  # Binary Cross Entropy\n","optimizador = optim.Adam(modelo.parameters(), lr=0.01)\n","\n","epocas = 200\n","\n","for epoca in range(epocas):\n","    modelo.train()\n","    salida = modelo(X)\n","    loss = criterio(salida, y)\n","\n","    optimizador.zero_grad()\n","    loss.backward()\n","    optimizador.step()\n","\n","    if (epoca + 1) % 10 == 0:\n","        print(f\"Ã‰poca {epoca+1}, PÃ©rdida: {loss.item():.4f}\")\n"],"metadata":{"id":"2epH-XRHb58g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##ğŸ§ª 7. EvaluaciÃ³n con frases nuevas\n","Probamos la red con frases que no estaban en el entrenamiento, para ver cÃ³mo generaliza."],"metadata":{"id":"YKZ5V6yhcDxI"}},{"cell_type":"code","source":["frases_prueba = [\n","    \"No me gustÃ³ la atenciÃ³n, bastante mala\",\n","    \"Muy buena experiencia, todo excelente\",\n","    \"Una estafa total, no lo recomiendo\",\n","    \"SÃºper conforme con el servicio\",\n","    \"Nada que ver con lo prometido, una decepciÃ³n\"\n","]\n","\n","# Vectorizamos las frases de prueba\n","X_prueba_np = np.array([vectorizar(frase, vocabulario) for frase in frases_prueba], dtype=np.float32)\n","X_prueba = torch.tensor(X_prueba_np)\n","\n","# PredicciÃ³n\n","modelo.eval()\n","with torch.no_grad():\n","    predicciones = modelo(X_prueba)\n","\n","# Mostrar resultados\n","for frase, pred in zip(frases_prueba, predicciones):\n","    clase = \"Positivo\" if pred.item() >= 0.5 else \"Negativo\"\n","    print(f\"Frase: '{frase}' => Sentimiento predicho: {clase} ({pred.item():.2f})\")"],"metadata":{"id":"gpIGfi-tcFPY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##ğŸ’¬ ReflexiÃ³n final\n","###ğŸ‘‰ Â¿QuÃ© aprendimos?\n","\n","* CÃ³mo implementar y entrenar una red neuronal multicapa para anÃ¡lisis de sentimiento.\n","\n","* CÃ³mo preprocesar texto en espaÃ±ol usando bag-of-words.\n","\n","* Las ventajas del MLP frente al perceptrÃ³n simple.\n","\n","* Limitaciones: aÃºn no capta el orden de las palabras ni el contexto secuencial.\n","\n","â¡ï¸ En la prÃ³xima actividad aprenderemos a usar redes recurrentes (LSTM) para incorporar secuencia y memoria en el procesamiento de texto. Â¡Nos acercamos a modelos mÃ¡s cercanos al lenguaje humano!"],"metadata":{"id":"138SwAUvcRnQ"}}]}