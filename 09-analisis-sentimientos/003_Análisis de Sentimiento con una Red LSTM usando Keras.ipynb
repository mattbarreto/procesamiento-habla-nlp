{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNmtH7QYPaFb/GuT7IumPxe"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#An√°lisis de Sentimiento con una Red LSTM usando Keras\n","##üéØ Objetivo\n","En esta actividad vas a construir un modelo de red neuronal recurrente (RNN), espec√≠ficamente una LSTM, usando la API Keras de TensorFlow. El modelo leer√° frases en espa√±ol y clasificar√° su sentimiento como positivo o negativo.\n","\n"],"metadata":{"id":"qSvHGspZdXSi"}},{"cell_type":"markdown","source":["##üß∞ 1. Preparaci√≥n del entorno\n","Importamos las librer√≠as necesarias. Si est√°s en Google Colab, pod√©s ejecutar la celda tal como est√°."],"metadata":{"id":"cy_gLJYldf5Q"}},{"cell_type":"code","source":["import numpy as np\n","from tensorflow import keras\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, LSTM, Dense\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences"],"metadata":{"id":"R4KyRLqWdfn3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##üóÇÔ∏è 2. Datos de entrenamiento\n","Vamos a usar las mismas frases que en la actividad anterior, pero ahora procesadas como secuencias de palabras, no como bolsa de palabras."],"metadata":{"id":"UpJwnXPUdtbA"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"KV8jghGzdSRj"},"outputs":[],"source":["frases = [\n","    \"La verdad, este lugar est√° b√°rbaro. Muy recomendable\",\n","    \"Una porquer√≠a de servicio, nunca m√°s vuelvo\",\n","    \"Me encant√≥ la comida, aunque la m√∫sica estaba muy fuerte\",\n","    \"El env√≠o fue lento y el producto lleg√≥ da√±ado. Qu√© desastre\",\n","    \"Todo excelente. Atenci√≥n de diez\",\n","    \"Qu√© estafa, me arrepiento de haber comprado\",\n","    \"Muy conforme con el resultado final\",\n","    \"No me gust√≥ para nada la experiencia\",\n","    \"Super√≥ mis expectativas, ¬°gracias!\",\n","    \"No lo recomiendo, mala calidad\"\n","]\n","\n","etiquetas = np.array([1, 0, 1, 0, 1, 0, 1, 0, 1, 0])"]},{"cell_type":"markdown","source":["##‚úèÔ∏è 3. Tokenizaci√≥n y vectorizaci√≥n\n","Con Keras, vamos a convertir las frases en secuencias de n√∫meros, donde cada n√∫mero representa una palabra del vocabulario."],"metadata":{"id":"fJ-RfP1kd3zw"}},{"cell_type":"code","source":["# Tokenizaci√≥n\n","tokenizer = Tokenizer(oov_token=\"<OOV>\")\n","tokenizer.fit_on_texts(frases)\n","\n","# Secuencias num√©ricas\n","secuencias = tokenizer.texts_to_sequences(frases)\n","\n","# Padding para que todas las frases tengan la misma longitud\n","maxlen = max(len(seq) for seq in secuencias)\n","X = pad_sequences(secuencias, maxlen=maxlen, padding='post')\n","\n","# Convertimos las etiquetas\n","y = np.array(etiquetas)"],"metadata":{"id":"IdBIn62nd3gA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##üß± 4. Definici√≥n del modelo LSTM\n","Vamos a usar una red con:\n","\n","* Capa de embedding (representaci√≥n vectorial aprendida de cada palabra).\n","\n","* Una capa LSTM para captar la secuencia.\n","\n","* Una capa densa para clasificar."],"metadata":{"id":"FfARMHgkeCUg"}},{"cell_type":"code","source":["# Par√°metros\n","vocab_size = len(tokenizer.word_index) + 1  # +1 por el token OOV\n","embedding_dim = 16\n","lstm_units = 32\n","\n","# Modelo\n","modelo = Sequential([\n","    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=maxlen),\n","    LSTM(units=lstm_units),\n","    Dense(1, activation='sigmoid')\n","])\n","\n","modelo.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","modelo.summary()"],"metadata":{"id":"qdBFUXe_eCsg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##üöÄ 5. Entrenamiento\n","Entrenamos el modelo por unas pocas √©pocas para ver resultados r√°pidos en clase. Si ten√©s m√°s tiempo o m√°s datos, pod√©s aumentarlas."],"metadata":{"id":"M3TtQP9lePlH"}},{"cell_type":"code","source":["modelo.fit(X, y, epochs=20, batch_size=2, verbose=1)\n"],"metadata":{"id":"IO9WfKXEePS3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##üß™ 6. Evaluaci√≥n con frases nuevas\n","Ahora vamos a probar el modelo con frases no vistas y observar sus predicciones."],"metadata":{"id":"KpqlBI4ReZv3"}},{"cell_type":"code","source":["frases_nuevas = [\n","    \"Muy buena atenci√≥n, qued√© encantado\",\n","    \"Horrible experiencia, no vuelvo m√°s\",\n","    \"Todo excelente, gracias por la atenci√≥n\",\n","    \"Me arrepiento completamente, fue un desastre\",\n","    \"Un servicio impecable y r√°pido\"\n","]\n","\n","# Tokenizamos y convertimos\n","secuencias_nuevas = tokenizer.texts_to_sequences(frases_nuevas)\n","X_nuevo = pad_sequences(secuencias_nuevas, maxlen=maxlen, padding='post')\n","\n","# Predicci√≥n\n","predicciones = modelo.predict(X_nuevo)\n","\n","# Mostrar resultados\n","for frase, pred in zip(frases_nuevas, predicciones):\n","    clase = \"Positivo\" if pred[0] >= 0.5 else \"Negativo\"\n","    print(f\"Frase: '{frase}' => Sentimiento predicho: {clase} ({pred[0]:.2f})\")\n"],"metadata":{"id":"Bq7pY6EUeZd_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#üß† Reflexi√≥n final\n","##üëâ ¬øQu√© aprendimos?\n","\n","* C√≥mo representar texto como secuencia de palabras usando Keras.\n","\n","* Qu√© es una red LSTM y c√≥mo recuerda el contexto.\n","\n","* C√≥mo el orden de las palabras s√≠ influye en el resultado.\n","\n","* Que las redes recurrentes pueden manejar frases m√°s complejas que los MLP o perceptrones."],"metadata":{"id":"TfghBPJPel0Y"}}]}