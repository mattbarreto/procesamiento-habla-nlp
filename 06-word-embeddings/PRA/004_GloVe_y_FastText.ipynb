{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1SC4rYzHZRIKZ5Bg8DMUXRyuwoQu9UWXC","authorship_tag":"ABX9TyO2pqo62f7iKLbGp4vEoVZb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Cuaderno Clase PLN - GloVe y FastText\n","\n","**Objetivos:**\n","*   Conocer enfoques alternativos a Word2Vec: GloVe (conteos globales) y FastText (subpalabras).\n","*   Entender la ventaja clave de FastText para manejar palabras fuera de vocabulario (OOV - Out Of Vocabulary).\n","*   Cargar y usar vectores FastText pre-entrenados.\n","*   Comparar resultados y capacidades (especialmente OOV) con Word2Vec.\n","*   Reflexionar sobre cómo evaluar la calidad de los embeddings y detectar sesgos.\n","\n","**Agenda:**\n","1.  Instalaciones e Importaciones\n","2.  Repaso Rápido: Word2Vec y sus limitaciones (OOV)\n","3.  GloVe: Vectores Globales desde Co-ocurrencias\n","4.  FastText: El Poder de las Subpalabras (¡Adiós OOV!)\n","5.  Cargando Vectores FastText Pre-entrenados\n","6.  Explorando FastText: Similitud, Analogías y ¡OOV!\n","7.  Comparativa: Word2Vec vs FastText (foco en OOV)\n","8.  Micro-Laboratorio (Ejercicio Práctico)\n","9.  Brainstorming: Evaluación y Detección de Sesgos"],"metadata":{"id":"Rzh9Sg9UeB_J"}},{"cell_type":"markdown","source":["# 1. Instalaciones e Importaciones"],"metadata":{"id":"CpBrBQ7-ejz2"}},{"cell_type":"code","source":["# Necesitamos gensim principalmente\n","!pip install gensim > /dev/null"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gG4_rcP3eXrs","executionInfo":{"status":"ok","timestamp":1746746123728,"user_tz":180,"elapsed":28715,"user":{"displayName":"BARRETO MATÍAS","userId":"15011433883683359534"}},"outputId":"04e3b377-54bd-428a-c672-49fd268e006d"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","source":["!pip uninstall gensim -y # Remove the existing gensim installation\n","!pip install gensim # Reinstall gensim to align with the NumPy version\n","# Restart the kernel to ensure the changes take effect"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L56qQY6vevNr","executionInfo":{"status":"ok","timestamp":1746746138974,"user_tz":180,"elapsed":6742,"user":{"displayName":"BARRETO MATÍAS","userId":"15011433883683359534"}},"outputId":"f4e51388-154f-4ee3-ca0d-0ed6bd089b6b"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: gensim 4.3.3\n","Uninstalling gensim-4.3.3:\n","  Successfully uninstalled gensim-4.3.3\n","Collecting gensim\n","  Using cached gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n","Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n","Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n","Using cached gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n","Installing collected packages: gensim\n","Successfully installed gensim-4.3.3\n"]}]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bwqWN5fAeBAl","executionInfo":{"status":"ok","timestamp":1746746218971,"user_tz":180,"elapsed":1478,"user":{"displayName":"BARRETO MATÍAS","userId":"15011433883683359534"}},"outputId":"e06cd2a7-bd30-48e3-fc19-61bf06ca318a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Librerías importadas.\n"]}],"source":["import gensim\n","from gensim.models import KeyedVectors, FastText # Ahora importamos FastText también\n","import numpy as np\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","print(\"Librerías importadas.\")\n","\n","# **Importante:** Para este cuaderno, idealmente necesitaremos:\n","# 1. El modelo Word2Vec cargado previamente (para comparar).\n","# 2. Un modelo FastText pre-entrenado para español (¡necesitamos descargarlo!)."]},{"cell_type":"markdown","source":["# 2. Repaso Rápido: Word2Vec y sus limitaciones (OOV)\n","\n","Recordemos:\n","*   Word2Vec (CBOW/Skip-gram) aprende embeddings prediciendo palabras en contextos locales.\n","*   Genera vectores densos que capturan semántica (similitud, analogías).\n","*   Usamos modelos pre-entrenados porque entrenarlos es costoso.\n","\n","**Una limitación importante:** Word2Vec asigna un vector a cada palabra *completa* que vio durante el entrenamiento. Si en tiempo de uso aparece una palabra que **NO estaba en el vocabulario original** (Out-Of-Vocabulary - OOV):\n","*   Un error tipográfico (\"computadorra\").\n","*   Una palabra nueva o muy rara (\"covid\", \"guasap\").\n","*   Una variante morfológica no vista (\"cantábamos\").\n","\n","**¿Qué pasa con Word2Vec?** ¡Generalmente da un error (`KeyError`) o asigna un vector genérico de \"desconocido\" (UNK - Unknown), perdiendo toda información específica! Esto es un problema en aplicaciones reales."],"metadata":{"id":"U0plow6ye_fl"}},{"cell_type":"markdown","source":["# 3. GloVe: Vectores Globales desde Co-ocurrencias\n","\n","GloVe (Global Vectors for Word Representation) de Stanford es otra técnica popular para obtener embeddings.\n","\n","*   **Enfoque Diferente:** En lugar de predecir contextos locales (como Word2Vec), GloVe se enfoca en las **estadísticas globales de co-ocurrencia** de palabras en todo el corpus.\n","*   **Idea Central:** La *relación* entre las probabilidades de co-ocurrencia de dos palabras con una tercera palabra \"sonda\" puede revelar información sobre su significado. Por ejemplo, la probabilidad de que \"hielo\" aparezca cerca de \"sólido\" será alta, mientras que cerca de \"gas\" será baja. La probabilidad de que \"vapor\" aparezca cerca de \"gas\" será alta y cerca de \"sólido\" baja. GloVe modela estas *proporciones* de probabilidades.\n","*   **Entrenamiento:** Construye una matriz gigante de co-ocurrencia (cuántas veces la palabra X aparece cerca de la palabra Y) y luego usa factorización de matrices para encontrar los vectores de palabras que mejor explican esa matriz.\n","*   **Resultado:** Embeddings densos similares a Word2Vec, a menudo muy buenos para tareas de similitud y analogía.\n","*   **Limitación OOV:** Al igual que Word2Vec, GloVe tradicionalmente opera a nivel de palabra completa, por lo que también sufre del problema de OOV.\n","\n","*(Nota: Encontrar modelos GloVe pre-entrenados y de buena calidad para español puede ser más difícil que para Word2Vec o FastText. A menudo se distribuyen en formato texto).*"],"metadata":{"id":"l8cCxcvmfc39"}},{"cell_type":"markdown","source":["# 4. FastText: El Poder de las Subpalabras (¡Adiós OOV!)\n","\n","FastText, desarrollado por Facebook AI Research (FAIR), es una extensión inteligente de Word2Vec (Skip-gram) que aborda directamente el problema OOV.\n","\n","*   **¡La Gran Idea!:** FastText no trata las palabras como unidades indivisibles. Representa cada palabra como una **bolsa de n-gramas de caracteres**, además de la palabra completa.\n","    *   Ejemplo: Para la palabra \"amarillo\" y usando n=3 (trigramas):\n","        *   Se consideran n-gramas como: `<am`, `ama`, `mar`, `ari`, `ril`, `ill`, `llo`, `lo>` (con `<` y `>` marcando inicio y fin).\n","        *   También se considera la palabra completa: `<amarillo>`\n","*   **Aprendizaje:** El modelo aprende vectores no solo para las palabras completas, sino **para todos los n-gramas de caracteres** vistos en el corpus.\n","*   **Vector Final:** El vector de una palabra se obtiene **sumando los vectores de todos sus n-gramas de caracteres** (y el vector de la palabra completa si existe).\n","\n","**¡La Ventaja Clave - Manejo de OOV!**\n","*   Si aparece una palabra nueva que no estaba en el vocabulario (ej: \"amarillento\"), FastText **aún puede construir un vector para ella**. ¿Cómo? Sumando los vectores de los n-gramas que sí conoce y que forman parte de la palabra nueva (ej: `<am`, `ama`, `mar`, `ari`, `ril`, `ill`, `lle`, `len`, `ent`, `nto`, `to>`).\n","*   El vector resultante captura información de las \"partes\" de la palabra, lo que a menudo resulta en una representación razonable incluso para palabras desconocidas.\n","\n","**Otros Beneficios:**\n","*   Funciona muy bien para **lenguajes morfológicamente ricos** (como el español) donde muchas palabras comparten raíces y afijos (ej: \"nación\", \"nacional\", \"nacionalidad\"). Comparten n-gramas, sus vectores estarán relacionados.\n","*   Es más robusto a **errores tipográficos** (\"computadorra\" compartirá muchos n-gramas con \"computadora\")."],"metadata":{"id":"94o6eSPaf5Fu"}},{"cell_type":"markdown","source":["# 5. Cargando Vectores FastText Pre-entrenados\n","\n","Al igual que con Word2Vec, lo usual es usar modelos FastText pre-entrenados.\n","\n","**Tarea:** Descargar los vectores pre-entrenados para **español** desde el sitio oficial de FastText: [https://fasttext.cc/docs/en/pretrained-vectors.html](https://fasttext.cc/docs/en/pretrained-vectors.html)\n","\n","*   **Importante:** Descargar el archivo `.bin`. Este formato contiene no solo los vectores de las palabras del vocabulario, sino también la información de los n-gramas necesaria para calcular vectores de palabras OOV. El formato `.vec` es más pequeño pero pierde esta capacidad OOV.\n","*   El archivo `.bin` para español también es grande (varios GB). Anoten la ruta donde lo guardan.\n","\n","**¡Reemplacen `'RUTA_AL_ARCHIVO_FASTTEXT.bin'` con la ruta real!**"],"metadata":{"id":"nU1XFPYAgZDl"}},{"cell_type":"code","source":["# --- ¡¡¡ MODIFICAR ESTA RUTA !!! ---\n","# Ejemplo de ruta (si lo subieron a Colab o está en el mismo directorio):\n","# path_to_fasttext = 'cc.es.300.bin' # Nombre común del archivo FastText para español\n","# Ejemplo de ruta (si está en Google Drive montado):\n","path_to_fasttext = '/content/drive/MyDrive/Clases/HABLA/006/PRA/vectors/wiki.es.bin' # Ajustar según su estructura"],"metadata":{"id":"_x3-luubmFb2","executionInfo":{"status":"ok","timestamp":1746746290300,"user_tz":180,"elapsed":14,"user":{"displayName":"BARRETO MATÍAS","userId":"15011433883683359534"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# Intentar cargar el modelo FastText\n","try:\n","    print(\"Cargando vectores FastText (.bin)... (¡Esto puede tardar MUCHO tiempo y consumir RAM!)\")\n","    # Usamos FastText.load_fasttext_format para cargar modelos .bin de FastText\n","    fasttext_model = gensim.models.fasttext.load_facebook_model(path_to_fasttext)\n","    # Los vectores están dentro del atributo .wv (como en Word2Vec cargado con KeyedVectors)\n","    fasttext_vectors = fasttext_model.wv\n","    print(f\"¡Vectores FastText cargados! Vocabulario (estimado): {len(fasttext_vectors.index_to_key)} palabras. Dimensión: {fasttext_vectors.vector_size}\")\n","    # Nota: El tamaño del vocabulario explícito puede ser menor en .bin, pero puede generar para OOV.\n","except FileNotFoundError:\n","    print(f\"Error: No se encontró el archivo FastText en la ruta '{path_to_fasttext}'.\")\n","    print(\"Por favor, descarga el archivo .bin pre-entrenado para español desde el sitio de FastText\")\n","    print(\"y asegúrate de que la variable 'path_to_fasttext' tenga la ruta correcta.\")\n","    fasttext_vectors = None\n","except Exception as e:\n","    print(f\"Ocurrió un error al cargar el modelo FastText: {e}\")\n","    fasttext_vectors = None"],"metadata":{"id":"LnkWSkrpmBT2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746746497296,"user_tz":180,"elapsed":180559,"user":{"displayName":"BARRETO MATÍAS","userId":"15011433883683359534"}},"outputId":"5d3c979c-23f4-4daf-b1f6-c2638215b51b"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Cargando vectores FastText (.bin)... (¡Esto puede tardar MUCHO tiempo y consumir RAM!)\n","¡Vectores FastText cargados! Vocabulario (estimado): 985667 palabras. Dimensión: 300\n"]}]},{"cell_type":"code","source":["# También intentemos recargar el modelo Word2Vec del martes para comparar\n","# --- ¡¡¡ MODIFICAR ESTA RUTA TAMBIÉN SI ES NECESARIO !!! ---\n","path_to_word2vec = '/content/drive/MyDrive/Clases/HABLA/006/PRA/vectors/SBW-vectors-300-min5.bin.gz' # La ruta del martes\n","word2vec_vectors = None"],"metadata":{"id":"KjLgGPmOmN8N","executionInfo":{"status":"ok","timestamp":1746746575199,"user_tz":180,"elapsed":13,"user":{"displayName":"BARRETO MATÍAS","userId":"15011433883683359534"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["try:\n","    print(\"\\nRecargando vectores Word2Vec del martes...\")\n","    word2vec_vectors = KeyedVectors.load_word2vec_format(path_to_word2vec, binary=True)\n","    print(\"Vectores Word2Vec recargados.\")\n","except Exception as e:\n","    print(f\"No se pudo recargar Word2Vec desde '{path_to_word2vec}': {e}\")"],"metadata":{"id":"L9YzR49AmKAf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746746636260,"user_tz":180,"elapsed":59399,"user":{"displayName":"BARRETO MATÍAS","userId":"15011433883683359534"}},"outputId":"bedf612e-2471-4314-91c8-70e1374093cb"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Recargando vectores Word2Vec del martes...\n","Vectores Word2Vec recargados.\n"]}]},{"cell_type":"markdown","source":["# 6. Explorando FastText: Similitud, Analogías y ¡OOV!\n","\n","Si `fasttext_vectors` se cargó, podemos hacer pruebas similares a las de Word2Vec, pero prestando especial atención a las palabras OOV."],"metadata":{"id":"7CZhEmSemSPQ"}},{"cell_type":"code","source":["if fasttext_vectors:\n","    # --- Pruebas estándar (similitud, analogía) ---\n","    print(\"\\n--- Explorando FastText ---\")\n","    # Similitud\n","    try:\n","        similares_rey_ft = fasttext_vectors.most_similar('rey', topn=5)\n","        print(\"FastText - Más similares a 'rey':\", similares_rey_ft)\n","    except KeyError:\n","        print(\"FastText - Palabra 'rey' no encontrada.\") # Raro si el modelo es bueno\n","\n","    # Analogía\n","    try:\n","        analogia_reina_ft = fasttext_vectors.most_similar(positive=['rey', 'mujer'], negative=['hombre'], topn=1)\n","        print(\"FastText - rey - hombre + mujer ≈\", analogia_reina_ft)\n","    except KeyError as e:\n","        print(f\"FastText - Error en analogía rey/reina: falta la palabra '{e.args[0]}'\")\n","\n","\n","    # --- ¡La prueba OOV! ---\n","    print(\"\\n--- Probando Palabras Fuera de Vocabulario (OOV) ---\")\n","    palabras_oov = [\n","        \"computadorra\", # Error tipográfico\n","        \"wasapear\",     # Neologismo / Palabra coloquial\n","        \"covid\",        # Palabra relativamente nueva (depende del corpus)\n","        \"programadorazo\", # Palabra inventada con sufijo común\n","        \"desayunábamos\", # Forma verbal menos frecuente\n","        \"internauta\"    # Palabra estándar, veamos si está\n","    ]\n","\n","    for palabra in palabras_oov:\n","        print(f\"\\nPalabra OOV: '{palabra}'\")\n","        # ¿Está explícitamente en el vocabulario? (Puede que sí si el corpus era reciente)\n","        in_vocab = palabra in fasttext_vectors\n","        print(f\"  ¿En vocabulario explícito? {in_vocab}\")\n","\n","        # Intentar obtener el vector (¡FastText debería poder!)\n","        try:\n","            vector_oov = fasttext_vectors[palabra]\n","            print(f\"  ¡Vector obtenido! (Dim: {vector_oov.shape})\")\n","\n","            # Encontrar similares basados en el vector generado\n","            similares_oov = fasttext_vectors.most_similar(palabra, topn=3)\n","            print(f\"  Similares (FastText): {similares_oov}\")\n","        except Exception as e:\n","            # Este error no debería ocurrir con un modelo .bin cargado correctamente\n","            print(f\"  Error inesperado obteniendo vector/similares para '{palabra}': {e}\")\n","\n","else:\n","    print(\"\\nNo se pudieron cargar los vectores FastText. Saltando la exploración.\")"],"metadata":{"id":"gUe0jsFLmTTc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746746746052,"user_tz":180,"elapsed":2687,"user":{"displayName":"BARRETO MATÍAS","userId":"15011433883683359534"}},"outputId":"75f7bdee-07b0-49c4-e566-9b1a4ff8122f"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Explorando FastText ---\n","FastText - Más similares a 'rey': [('monarca', 0.7459232807159424), ('rey―', 0.6321003437042236), ('exmonarca', 0.6310685873031616), ('rey,', 0.6303314566612244), ('reina', 0.6260297298431396)]\n","FastText - rey - hombre + mujer ≈ [('reina', 0.6612293720245361)]\n","\n","--- Probando Palabras Fuera de Vocabulario (OOV) ---\n","\n","Palabra OOV: 'computadorra'\n","  ¿En vocabulario explícito? True\n","  ¡Vector obtenido! (Dim: (300,))\n","  Similares (FastText): [('computador', 0.8841244578361511), ('computadora/ordenador', 0.8638773560523987), ('computadora,', 0.8576205372810364)]\n","\n","Palabra OOV: 'wasapear'\n","  ¿En vocabulario explícito? True\n","  ¡Vector obtenido! (Dim: (300,))\n","  Similares (FastText): [('dissapear', 0.6578547358512878), ('tapear', 0.6350954174995422), ('papear', 0.5647345781326294)]\n","\n","Palabra OOV: 'covid'\n","  ¿En vocabulario explícito? True\n","  ¡Vector obtenido! (Dim: (300,))\n","  Similares (FastText): [('covides', 0.7112458944320679), ('coview', 0.6201539635658264), ('covida', 0.5898890495300293)]\n","\n","Palabra OOV: 'programadorazo'\n","  ¿En vocabulario explícito? True\n","  ¡Vector obtenido! (Dim: (300,))\n","  Similares (FastText): [('programadora', 0.8951981663703918), ('programadoras', 0.8484038710594177), ('programadorcccp', 0.8139643669128418)]\n","\n","Palabra OOV: 'desayunábamos'\n","  ¿En vocabulario explícito? True\n","  ¡Vector obtenido! (Dim: (300,))\n","  Similares (FastText): [('dábamos', 0.7762657999992371), ('tomábamos', 0.7618230581283569), ('grabábamos', 0.7587261199951172)]\n","\n","Palabra OOV: 'internauta'\n","  ¿En vocabulario explícito? True\n","  ¡Vector obtenido! (Dim: (300,))\n","  Similares (FastText): [('internautas', 0.8382586240768433), ('internaute', 0.8171950578689575), ('cibernauta', 0.7792933583259583)]\n"]}]},{"cell_type":"markdown","source":["# 7. Comparativa: Word2Vec vs FastText (foco en OOV)\n","\n","Ahora, si ambos modelos (`word2vec_vectors` y `fasttext_vectors`) están cargados, podemos comparar directamente su comportamiento con palabras OOV."],"metadata":{"id":"IvE8aywDmXio"}},{"cell_type":"code","source":["# --- Comparación directa OOV ---\n","if word2vec_vectors and fasttext_vectors:\n","    print(\"\\n--- Comparación OOV: Word2Vec vs FastText ---\")\n","\n","    # Reutilizamos la lista de palabras OOV\n","    palabras_oov = [\n","        \"computadorra\", \"wasapear\", \"covid\", \"programadorazo\", \"desayunábamos\", \"internauta\"\n","    ]\n","\n","    for palabra in palabras_oov:\n","        print(f\"\\n--- Palabra: '{palabra}' ---\")\n","\n","        # Intentar con Word2Vec\n","        print(\"  Intentando con Word2Vec:\")\n","        try:\n","            vector_w2v = word2vec_vectors[palabra]\n","            similares_w2v = word2vec_vectors.most_similar(palabra, topn=3)\n","            print(f\"    ¡Encontrada! Vector: {vector_w2v.shape}, Similares: {similares_w2v}\")\n","        except KeyError:\n","            print(\"    ERROR: Palabra no encontrada en el vocabulario Word2Vec (KeyError).\")\n","        except Exception as e:\n","            print(f\"    ERROR inesperado con Word2Vec: {e}\")\n","\n","        # Intentar con FastText\n","        print(\"  Intentando con FastText:\")\n","        try:\n","            vector_ft = fasttext_vectors[palabra]\n","            similares_ft = fasttext_vectors.most_similar(palabra, topn=3)\n","            print(f\"    ¡Vector generado! Vector: {vector_ft.shape}, Similares: {similares_ft}\")\n","        except Exception as e:\n","            # No debería fallar por KeyError, pero podría haber otro error\n","            print(f\"    ERROR inesperado con FastText: {e}\")\n","\n","else:\n","    print(\"\\nNo se pudieron cargar ambos modelos (Word2Vec y FastText) para comparar.\")"],"metadata":{"id":"Znmbfy7BmZgf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746746921882,"user_tz":180,"elapsed":8717,"user":{"displayName":"BARRETO MATÍAS","userId":"15011433883683359534"}},"outputId":"45777b09-873b-460d-f70c-6067991d74ef"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Comparación OOV: Word2Vec vs FastText ---\n","\n","--- Palabra: 'computadorra' ---\n","  Intentando con Word2Vec:\n","    ERROR: Palabra no encontrada en el vocabulario Word2Vec (KeyError).\n","  Intentando con FastText:\n","    ¡Vector generado! Vector: (300,), Similares: [('computador', 0.8841244578361511), ('computadora/ordenador', 0.8638773560523987), ('computadora,', 0.8576205372810364)]\n","\n","--- Palabra: 'wasapear' ---\n","  Intentando con Word2Vec:\n","    ERROR: Palabra no encontrada en el vocabulario Word2Vec (KeyError).\n","  Intentando con FastText:\n","    ¡Vector generado! Vector: (300,), Similares: [('dissapear', 0.6578547358512878), ('tapear', 0.6350954174995422), ('papear', 0.5647345781326294)]\n","\n","--- Palabra: 'covid' ---\n","  Intentando con Word2Vec:\n","    ERROR: Palabra no encontrada en el vocabulario Word2Vec (KeyError).\n","  Intentando con FastText:\n","    ¡Vector generado! Vector: (300,), Similares: [('covides', 0.7112458944320679), ('coview', 0.6201539635658264), ('covida', 0.5898890495300293)]\n","\n","--- Palabra: 'programadorazo' ---\n","  Intentando con Word2Vec:\n","    ERROR: Palabra no encontrada en el vocabulario Word2Vec (KeyError).\n","  Intentando con FastText:\n","    ¡Vector generado! Vector: (300,), Similares: [('programadora', 0.8951981663703918), ('programadoras', 0.8484038710594177), ('programadorcccp', 0.8139643669128418)]\n","\n","--- Palabra: 'desayunábamos' ---\n","  Intentando con Word2Vec:\n","    ¡Encontrada! Vector: (300,), Similares: [('charlábamos', 0.7706557512283325), ('cenábamos', 0.7697142958641052), ('despertábamos', 0.769340455532074)]\n","  Intentando con FastText:\n","    ¡Vector generado! Vector: (300,), Similares: [('dábamos', 0.7762657999992371), ('tomábamos', 0.7618230581283569), ('grabábamos', 0.7587261199951172)]\n","\n","--- Palabra: 'internauta' ---\n","  Intentando con Word2Vec:\n","    ¡Encontrada! Vector: (300,), Similares: [('cibernauta', 0.6111046671867371), ('Tianya', 0.5805689692497253), ('blogger', 0.5799006819725037)]\n","  Intentando con FastText:\n","    ¡Vector generado! Vector: (300,), Similares: [('internautas', 0.8382586240768433), ('internaute', 0.8171950578689575), ('cibernauta', 0.7792933583259583)]\n"]}]},{"cell_type":"markdown","source":["**Conclusión de la Comparativa:**\n","\n","Deberían observar que para la mayoría (o todas) las palabras OOV, Word2Vec lanza un `KeyError`, mientras que FastText logra generar un vector y encontrar palabras similares (que pueden o no ser muy coherentes, dependiendo de qué tan \"rara\" sea la palabra OOV, pero *intenta* algo basado en sus partes).\n","\n","**¿Cuándo usar FastText?**\n","*   Cuando trabajas con texto ruidoso (redes sociales, OCR) con typos.\n","*   Cuando tratas con lenguajes morfológicamente ricos (español, alemán, etc.).\n","*   Cuando esperas encontrar palabras nuevas o raras en tus datos de aplicación.\n","*   **En general, para español, FastText suele ser una opción más robusta y recomendable que Word2Vec o GloVe.**\n","\n","**Desventaja:** Los modelos `.bin` de FastText son más grandes en disco y consumen más RAM porque almacenan información de los n-gramas."],"metadata":{"id":"uU57C8I8mcdZ"}},{"cell_type":"markdown","source":["# 8. Micro-Laboratorio (Ejercicio Práctico)\n","\n","**Consigna:** (Asumiendo que `fasttext_vectors` y `word2vec_vectors` están cargados)\n","\n","1.  **Comparación de Resultados:**\n","    *   Elegir 3 palabras que **sí** estén en ambos vocabularios (ej: 'gato', 'correr', 'inteligencia').\n","    *   Para cada palabra, obtener las 5 más similares usando `word2vec_vectors.most_similar()` y `fasttext_vectors.most_similar()`.\n","    *   Comparar las listas de similares. ¿Son idénticas? ¿Muy parecidas? ¿Diferentes? ¿Cuál les parece \"mejor\" o más coherente? Anotar observaciones.\n","\n","2.  **Test OOV Exhaustivo:**\n","    *   Crear una lista propia de 10 palabras OOV. Incluyan:\n","        *   Errores tipográficos comunes (ej: \"hobmre\", \"qeu\", \"dicimbre\").\n","        *   Diminutivos/Aumentativos (ej: \"perrito\", \"casita\", \"libraco\").\n","        *   Formas verbales conjugadas (ej: \"habíamos comido\", \"cantasteis\").\n","        *   Palabras inventadas pero plausibles (ej: \"tecnoestrés\", \"computofilia\").\n","    *   Para **cada** palabra OOV de su lista:\n","        *   Verificar si da `KeyError` en `word2vec_vectors`.\n","        *   Obtener las 3 palabras más similares usando `fasttext_vectors`. Anotar los resultados. ¿Los similares que da FastText tienen algún sentido basado en las partes de la palabra OOV?\n","\n","3.  **Discusión:**\n","    *   ¿En qué tipo de aplicación real (ej: un chatbot de atención al cliente, un sistema de recomendación de noticias, un corrector ortográfico) creen que la capacidad OOV de FastText marcaría una diferencia significativa respecto a usar Word2Vec? ¿Por qué?"],"metadata":{"id":"6bPYX6xPmdU0"}},{"cell_type":"code","source":[],"metadata":{"id":"26vXMK1amgTI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 9. Brainstorming: Evaluación y Detección de Sesgos\n","\n","Hemos visto diferentes formas de crear embeddings (Word2Vec, GloVe, FastText) y cómo usarlos. Pero, ¿cómo sabemos si un conjunto de embeddings es \"bueno\"? ¿Y cómo abordamos el problema de los sesgos que mencionamos en clases anteriores?\n","\n","**Pregunta:** **¿Cómo podemos evaluar la calidad de los word embeddings y detectar posibles sesgos?**\n","\n","*   **Evaluación Intrínseca:**\n","    *   Medir qué tan bien funcionan los embeddings en tareas específicas relacionadas con las palabras mismas, **sin** una aplicación final.\n","    *   Ejemplos:\n","        *   **Similitud de Palabras:** Comparar la similitud coseno entre pares de palabras según los embeddings, con juicios de similitud dados por humanos (datasets estándar como WordSim-353, SimLex-999, adaptados o creados para español).\n","        *   **Tareas de Analogía:** Ver qué tan bien resuelven analogías (`rey - hombre + mujer = ?`). Hay datasets estándar de analogías (Google Analogies, BATS).\n","*   **Evaluación Extrínseca:**\n","    *   Usar los embeddings como **características de entrada (features)** para un modelo de PLN más complejo que resuelve una tarea final (downstream task).\n","    *   Ejemplos: Clasificación de sentimientos, reconocimiento de entidades nombradas, clasificación de temas.\n","    *   Medir si usar estos embeddings mejora el rendimiento (precisión, F1-score, etc.) del sistema final comparado con no usarlos o usar otros embeddings. **Esta suele ser la evaluación más relevante en la práctica.**\n","*   **Detección de Sesgos:**\n","    *   **Analogías Específicas:** Crear analogías diseñadas para revelar sesgos sociales (género-profesión, raza-sentimiento, etc.). `programador - hombre + mujer = ?`.\n","    *   **Pruebas de Asociación Implícita (como WEAT - Word Embedding Association Test):** Miden matemáticamente qué tan asociados están ciertos grupos de palabras (ej: nombres masculinos vs femeninos) con ciertos atributos (ej: carreras científicas vs artísticas, adjetivos positivos vs negativos) dentro del espacio de embeddings.\n","    *   **Visualización:** A veces, proyectar grupos específicos (hombres/mujeres, profesiones) a 2D puede revelar agrupaciones o direcciones sesgadas.\n","    *   **Auditoría de Vecinos Cercanos:** Ver los `most_similar` para palabras sensibles.\n","\n","**Una vez detectado el sesgo, ¿qué hacemos?** ¿Existen técnicas para mitigarlo (\"debiasing\")? ¿Es mejor buscar/crear corpus menos sesgados? ¿O simplemente ser transparentes sobre los sesgos del modelo?\n","\n","**(Discusión en grupo)**"],"metadata":{"id":"x0BNPC_9minX"}}]}