{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1qtR8po4qz4errEytX3KgoAsggrwO67kl","authorship_tag":"ABX9TyMy1ojKtk/5u6a7M2TdV0g+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Cuerno Clase PLN - Introducción a Word Embeddings y Word2Vec\n","\n","**Objetivos:**\n","*   Entender las limitaciones de BoW/TF-IDF.\n","*   Comprender el concepto de Word Embeddings (vectores densos semánticos).\n","*   Conocer las arquitecturas Word2Vec (CBOW y Skip-gram).\n","*   Aprender a cargar y usar vectores Word2Vec pre-entrenados con `gensim`.\n","*   Explorar similitud y analogías entre palabras.\n","*   (Opcional) Visualizar embeddings.\n","\n","**Agenda:**\n","1.  Instalaciones e Importaciones\n","2.  ¿Por qué necesitamos algo mejor que BoW/TF-IDF?\n","3.  La Idea Clave: Word Embeddings\n","4.  Word2Vec: Aprendiendo de los Vecinos (CBOW y Skip-gram)\n","5.  Usando Vectores Pre-entrenados (¡Importante!)\n","6.  Cargando Vectores con Gensim\n","7.  Explorando los Vectores: Similitud y Analogías\n","8.  (Opcional) Visualización de Embeddings\n","9.  Micro-Laboratorio (Ejercicio Práctico)\n","10. Brainstorming: Sesgos en Embeddings"],"metadata":{"id":"hScKD4A12Eky"}},{"cell_type":"markdown","source":["# 1. Instalaciones e Importaciones"],"metadata":{"id":"NufE2aqOeofZ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Idwdlzs42Aew"},"outputs":[],"source":["# Gensim es la librería clave para trabajar con Word2Vec, GloVe, FastText\n","!pip install gensim matplotlib seaborn scikit-learn > /dev/null"]},{"cell_type":"code","source":["!pip uninstall gensim -y # Remove the existing gensim installation\n","!pip install gensim # Reinstall gensim to align with the NumPy version\n","# Restart the kernel to ensure the changes take effect"],"metadata":{"id":"aRxOgRsAZ9g-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import gensim\n","from gensim.models import KeyedVectors # Para cargar modelos pre-entrenados\n","import numpy as np\n","import warnings\n","warnings.filterwarnings('ignore') # Para evitar warnings de gensim a veces\n","\n","# Para visualización (Opcional)\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.decomposition import PCA\n","from sklearn.manifold import TSNE\n","\n","print(\"Librerías importadas.\")\n","# Nota: Más adelante necesitaremos descargar un archivo de vectores pre-entrenados."],"metadata":{"id":"NJC-y1t-2h1U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 2. ¿Por qué necesitamos algo mejor que BoW/TF-IDF?\n","\n","Recordemos rápido las limitaciones de las representaciones que vimos previamente:\n","\n","*   **Vectores Enormes y Dispersos:** Si tenemos 50,000 palabras únicas, cada documento es un vector de 50,000 dimensiones, ¡casi todo ceros! Muy ineficiente.\n","*   **No Capturan Significado (Semántica):**\n","    *   Las palabras \"coche\" y \"auto\" son sinónimos, pero para BoW/TF-IDF son tan diferentes entre sí como lo son de \"banana\". Cada una es una columna distinta.\n","    *   No hay noción de que \"perro\" y \"gato\" son más parecidos entre sí que \"perro\" y \"mesa\".\n","*   **Ignoran el Orden de las Palabras:** \"Perro muerde a hombre\" y \"Hombre muerde a perro\" tienen representaciones muy similares (mismas palabras, distintos conteos quizás, pero misma bolsa).\n","\n","Necesitamos una forma de representar palabras que capture su **significado** y las **relaciones** entre ellas."],"metadata":{"id":"azgmHuDI5urj"}},{"cell_type":"markdown","source":["# 3. La Idea Clave: Word Embeddings\n","\n","¡Aquí entran los **Word Embeddings** (incrustaciones de palabras)!\n","\n","*   **Idea Central:** Representar cada palabra como un **vector denso** (la mayoría de valores NO son cero) en un espacio de **menor dimensión** (ej: 50, 100, 300 dimensiones, ¡no 50,000!).\n","*   **La \"Magia\":** Estos vectores no son aleatorios. Se aprenden de grandes cantidades de texto de forma que palabras con **significados similares** terminan teniendo **vectores cercanos** en ese espacio vectorial.\n","    *   El vector de \"gato\" estará cerca del vector de \"perro\".\n","    *   El vector de \"contento\" estará cerca del vector de \"feliz\".\n","*   **Analogía:** Imaginen un mapa. Cada ciudad es una palabra. Las ciudades cercanas en el mapa representan palabras semánticamente relacionadas.\n","*   **¡Álgebra de Palabras!** Lo más sorprendente es que la estructura de este espacio vectorial captura relaciones semánticas y sintácticas de forma lineal. El ejemplo clásico:\n","    *   `vector('Rey') - vector('Hombre') + vector('Mujer') ≈ vector('Reina')`\n","    *   Podemos \"operar\" con los significados de las palabras usando sus vectores."],"metadata":{"id":"aX00cMqM6DaX"}},{"cell_type":"markdown","source":["# 4. Word2Vec: Aprendiendo de los Vecinos (CBOW y Skip-gram)\n","\n","Word2Vec (Mikolov et al., Google, 2013) fue uno de los métodos pioneros y más populares para aprender estos embeddings.\n","\n","*   **Principio Fundamental (Hipótesis Distribucional):** *Una palabra se caracteriza por las compañías que mantiene*. Es decir, aprendemos el significado de una palabra observando las palabras que aparecen a su alrededor (su contexto).\n","*   **Arquitecturas (Redes Neuronales Superficiales):**\n","    *   **CBOW (Continuous Bag-of-Words):** Dado un conjunto de palabras de contexto (ej: las 2 palabras antes y las 2 después), intenta **predecir la palabra central**. Es más rápido de entrenar y bueno para palabras frecuentes.\n","        *   `[la, casa, ___, bonita] -> predecir 'es'`\n","    *   **Skip-gram:** Dada la palabra central, intenta **predecir las palabras de su contexto**. Es más lento pero generalmente funciona mejor con palabras infrecuentes y captura mejor relaciones semánticas más finas. Suele ser el preferido si el rendimiento es clave.\n","        *   `'es' -> predecir [la, casa, ___, bonita]`\n","*   **¿Cómo se aprende?:** La red neuronal (muy simple, usualmente una sola capa oculta) ajusta los vectores (embeddings) de entrada y salida para mejorar en la tarea de predicción (sea CBOW o Skip-gram). Los detalles técnicos (Negative Sampling, Hierarchical Softmax) son optimizaciones para hacerlo eficiente, pero la idea es que los vectores se ajustan para que palabras que aparecen en contextos similares terminen con vectores similares.\n","*   **Resultado:** Una \"tabla de consulta\" (o diccionario) donde cada palabra del vocabulario tiene asociado un vector denso aprendido (el embedding)."],"metadata":{"id":"udidG2ZG6WFe"}},{"cell_type":"markdown","source":["# 5. Usando Vectores Pre-entrenados (¡Importante!)\n","\n","Entrenar modelos Word2Vec (o GloVe, FastText) desde cero requiere:\n","1.  **Muchísimo texto:** Millones o miles de millones de palabras (ej: toda la Wikipedia, noticias de años).\n","2.  **Mucho tiempo computacional:** Horas o días, incluso con hardware potente.\n","\n","**¡La buena noticia!** Investigadores y empresas ya han hecho este trabajo pesado y publican los **modelos pre-entrenados**. Nosotros podemos simplemente descargarlos y usarlos.\n","\n","Para español, una fuente común y de buena calidad son los vectores entrenados sobre el **Spanish Billion Words Corpus (SBWC)** por la Universidad de Chile. También hay otros, como los de FastText.\n","\n","**Tarea:** Necesitamos descargar uno de estos archivos. Busquen en la web \"Spanish Word Embeddings SBWC download\" o similar.\n","*   Recomendado: Descargar el archivo `.bin` (formato binario, más compacto) si está disponible. Si no, el `.vec` (formato texto). Suelen ser archivos grandes (varios GB!).\n","*   **Importante:** Una vez descargado, anoten la ruta completa donde guardaron el archivo en su máquina o Google Drive.\n","\n","[Spanish Word Embeddings SBWC download](https://github.com/dccuchile/spanish-word-embeddings?tab=readme-ov-file) [Mirror](https://drive.google.com/file/d/1rSI0q8J_USo1GpPfzUOa7pNgmLc66svv/edit)"],"metadata":{"id":"LP0X-ilR6WCY"}},{"cell_type":"markdown","source":["# 6. Cargando Vectores con Gensim\n","\n","Una vez que tenemos el archivo (`.bin` o `.vec`), usamos `gensim` para cargarlo.\n","\n","**¡Reemplacen `'RUTA_AL_ARCHIVO_DESCARGADO.bin'` con la ruta real donde guardaron el archivo!**\n","\n","Si están en Colab, pueden subir el archivo a su sesión (tardará si es grande) o montarlo desde Google Drive."],"metadata":{"id":"cooGKzHS6V-4"}},{"cell_type":"code","source":["# --- ¡¡¡ MODIFICAR ESTA RUTA !!! ---\n","# Ejemplo de ruta (si lo subieron a Colab o está en el mismo directorio):\n","# path_to_vectors = 'SBW-vectors-300-min5.bin.gz' # Reemplazar con el nombre exacto\n","# Ejemplo de ruta (si está en Google Drive montado en /content/drive/MyDrive/):\n","# path_to_vectors = '/content/drive/MyDrive/Colab Notebooks/embeddings/SBW-vectors-300-min5.bin.gz' # Ajustar según su estructura de Drive\n","path_to_vectors = '/content/drive/MyDrive/Clases/HABLA/006/PRA/vectors/SBW-vectors-300-min5.bin.gz' # Reemplazar con el nombre exacto"],"metadata":{"id":"up9W-fYrKWWp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Intentar cargar el modelo\n","try:\n","    # Si es formato binario (.bin, .bin.gz): binary=True\n","    # Si es formato texto (.vec, .txt, .vec.gz): binary=False\n","    print(\"Cargando vectores... (puede tardar unos minutos)\")\n","    word_vectors = KeyedVectors.load_word2vec_format(path_to_vectors, binary=True)\n","    # Si da error con binary=True, y tu archivo es .vec o .txt, prueba con binary=False\n","    # word_vectors = KeyedVectors.load_word2vec_format(path_to_vectors, binary=False)\n","    print(f\"¡Vectores cargados! Vocabulario: {len(word_vectors.index_to_key)} palabras. Dimensión: {word_vectors.vector_size}\")\n","except FileNotFoundError:\n","    print(f\"Error: No se encontró el archivo en la ruta '{path_to_vectors}'.\")\n","    print(\"Por favor, descarga los vectores pre-entrenados (ej: SBWC)\")\n","    print(\"y asegúrate de que la variable 'path_to_vectors' tenga la ruta correcta.\")\n","    word_vectors = None # Dejarlo como None si no se pudo cargar"],"metadata":{"id":"AMvrBByh53cq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 7. Explorando los Vectores: Similitud y Analogías\n","\n","¡Ahora viene lo divertido! Si los vectores se cargaron (`word_vectors` no es `None`), podemos explorarlos."],"metadata":{"id":"P8qDLgY7Q9JD"}},{"cell_type":"markdown","source":["Verificación de existencia en el vocabulario"],"metadata":{"id":"xvnGdmrkSBXX"}},{"cell_type":"code","source":["if 'word_vectors' in locals() and word_vectors:\n","    print(\"Explorando las propiedades de los vectores de palabras...\")\n","\n","    # Verificar si una palabra está en el vocabulario\n","    print(f\"\\n'rey' in vocabulario? {'rey' in word_vectors}\")\n","    print(f\"'programacion' in vocabulario? {'programacion' in word_vectors}\") # Ojo: puede estar 'programación'\n","    print(f\"'programación' in vocabulario? {'programación' in word_vectors}\")\n","    print(f\"'casa' in vocabulario? {'casa' in word_vectors}\")\n","    print(f\"'qwerty' in vocabulario? {'qwerty' in word_vectors}\") # Probablemente no\n","else:\n","    print(\"\\nLa variable 'word_vectors' no está definida o está vacía. Por favor, carga el modelo de vectores de palabras primero.\")"],"metadata":{"id":"R9Tgwu4YQ90_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Obtener vector de una palabra"],"metadata":{"id":"Ig16VqvDSCaB"}},{"cell_type":"code","source":["if 'word_vectors' in locals() and word_vectors:\n","    # Obtener el vector de una palabra\n","    if 'rey' in word_vectors:\n","        vector_rey = word_vectors['rey']\n","        print(f\"\\nDimensiones del vector 'rey': {vector_rey.shape}\")\n","        # print(\"Primeros 10 valores:\", vector_rey[:10]) # Descomentar para ver un trozo\n","    else:\n","         print(\"\\nLa palabra 'rey' no se encuentra en el vocabulario.\")\n","else:\n","    print(\"\\nLa variable 'word_vectors' no está definida o está vacía. Esta celda requiere que el modelo esté cargado.\")"],"metadata":{"id":"cs4ozCOkR7mz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[" Encontrar palabras similares"],"metadata":{"id":"sFQXrNyPSGn0"}},{"cell_type":"code","source":["if 'word_vectors' in locals() and word_vectors:\n","    print(\"\\n--- Palabras más similares ---\")\n","    try:\n","        similares_rey = word_vectors.most_similar('rey', topn=5)\n","        print(\"Más similares a 'rey':\", similares_rey)\n","    except KeyError:\n","        print(\"Palabra 'rey' no encontrada.\")\n","\n","    try:\n","        similares_perro = word_vectors.most_similar('perro', topn=5)\n","        print(\"Más similares a 'perro':\", similares_perro)\n","    except KeyError:\n","        print(\"Palabra 'perro' no encontrada.\")\n","else:\n","    print(\"\\nLa variable 'word_vectors' no está definida o está vacía. Esta celda requiere que el modelo esté cargado.\")"],"metadata":{"id":"Isgivtu-b6vU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if 'word_vectors' in locals() and word_vectors:\n","    print(\"\\n--- Palabras más similares ---\")\n","    try:\n","        # Probar con una palabra que podría no estar exacta: 'computadora' vs 'computador'\n","        palabra_compu = 'computadora' if 'computadora' in word_vectors else ('computador' if 'computador' in word_vectors else None)\n","        if palabra_compu:\n","            similares_compu = word_vectors.most_similar(palabra_compu, topn=5)\n","            print(f\"Más similares a '{palabra_compu}':\", similares_compu)\n","        else:\n","            print(\"No se encontró 'computadora' ni 'computador'.\")\n","    except KeyError:\n","        print(\"Error buscando similares a computadora/or.\")"],"metadata":{"id":"r6KKEre6cGPA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Realizar analogías (Álgebra de palabras)"],"metadata":{"id":"eT7FmwLESMgz"}},{"cell_type":"code","source":["if 'word_vectors' in locals() and word_vectors:\n","    print(\"\\n--- Analogías ---\")\n","    # Analogía clásica: rey - hombre + mujer = ? (esperamos reina)\n","    try:\n","        # Verificar si las palabras necesarias existen antes de la analogía\n","        if all(p in word_vectors for p in ['rey', 'hombre', 'mujer']):\n","            analogia_reina = word_vectors.most_similar(positive=['rey', 'mujer'], negative=['hombre'], topn=1)\n","            print(\"rey - hombre + mujer ≈\", analogia_reina)\n","        else:\n","             missing_words = [p for p in ['rey', 'hombre', 'mujer'] if p not in word_vectors]\n","             print(f\"Faltan palabras para la analogía rey/reina: {missing_words}\")\n","    except KeyError as e:\n","        print(f\"Error en analogía rey/reina: falta la palabra '{e.args[0]}'\")\n","    except Exception as e:\n","        print(f\"Ocurrió un error inesperado en la analogía rey/reina: {e}\")\n","\n","\n","    # Analogía de capitales: España - Madrid + París = ? (esperamos Francia)\n","    try:\n","        # Ojo: verificar que las palabras estén! Usar .lower() si es necesario\n","        palabras_capitales = ['españa', 'madrid', 'parís'] # Usar minúsculas si el modelo es cased\n","        if all(p in word_vectors for p in palabras_capitales):\n","            analogia_francia = word_vectors.most_similar(positive=['españa', 'parís'], negative=['madrid'], topn=1)\n","            print(\"España - Madrid + París ≈\", analogia_francia)\n","        else:\n","            missing_words = [p for p in palabras_capitales if p not in word_vectors]\n","            print(f\"Faltan palabras para la analogía de capitales: {missing_words}\")\n","    except KeyError as e:\n","        print(f\"Error en analogía capitales: falta la palabra '{e.args[0]}'\")\n","    except Exception as e:\n","        print(f\"Ocurrió un error inesperado en la analogía capitales: {e}\")\n","\n","\n","    # Analogía de verbos: correr - corriendo + comiendo = ? (esperamos comer)\n","    try:\n","        palabras_verbos = ['correr', 'corriendo', 'comiendo']\n","        if all(p in word_vectors for p in palabras_verbos):\n","           analogia_comer = word_vectors.most_similar(positive=['correr', 'comiendo'], negative=['corriendo'], topn=1)\n","           print(\"correr - corriendo + comiendo ≈\", analogia_comer)\n","        else:\n","            missing_words = [p for p in palabras_verbos if p not in word_vectors]\n","            print(f\"Faltan palabras para la analogía de verbos: {missing_words}\")\n","    except KeyError as e:\n","        print(f\"Error en analogía verbos: falta la palabra '{e.args[0]}'\")\n","    except Exception as e:\n","        print(f\"Ocurrió un error inesperado en la analogía verbos: {e}\")\n","\n","else:\n","    print(\"\\nLa variable 'word_vectors' no está definida o está vacía. Esta celda requiere que el modelo esté cargado.\")"],"metadata":{"id":"LJltoUTsR7hE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Calcular similitud coseno"],"metadata":{"id":"YWNV-7GLSRuF"}},{"cell_type":"code","source":["if 'word_vectors' in locals() and word_vectors:\n","    print(\"\\n--- Similitud Coseno ---\")\n","    try:\n","        if 'perro' in word_vectors and 'gato' in word_vectors:\n","            sim_perro_gato = word_vectors.similarity('perro', 'gato')\n","            print(f\"Similitud('perro', 'gato'): {sim_perro_gato:.4f}\")\n","        else:\n","            print(\"Faltan 'perro' o 'gato' para calcular similitud.\")\n","    except KeyError:\n","        print(\"Error al calcular similitud entre 'perro' y 'gato' (KeyError).\") # Manejo redundante si se verifica antes\n","    except Exception as e:\n","        print(f\"Ocurrió un error inesperado calculando similitud perro/gato: {e}\")\n","\n","\n","    try:\n","        if 'perro' in word_vectors and 'mesa' in word_vectors:\n","            sim_perro_mesa = word_vectors.similarity('perro', 'mesa')\n","            print(f\"Similitud('perro', 'mesa'): {sim_perro_mesa:.4f}\")\n","        else:\n","            print(\"Faltan 'perro' o 'mesa' para calcular similitud.\")\n","    except KeyError:\n","        print(\"Error al calcular similitud entre 'perro' y 'mesa' (KeyError).\")\n","    except Exception as e:\n","        print(f\"Ocurrió un error inesperado calculando similitud perro/mesa: {e}\")\n","\n","    try:\n","        if 'feliz' in word_vectors and 'contento' in word_vectors:\n","            sim_feliz_contento = word_vectors.similarity('feliz', 'contento')\n","            print(f\"Similitud('feliz', 'contento'): {sim_feliz_contento:.4f}\")\n","        else:\n","            print(\"Faltan 'feliz' o 'contento' para calcular similitud.\")\n","    except KeyError:\n","        print(\"Error al calcular similitud entre 'feliz' y 'contento' (KeyError).\")\n","    except Exception as e:\n","        print(f\"Ocurrió un error inesperado calculando similitud feliz/contento: {e}\")\n","\n","else:\n","    print(\"\\nLa variable 'word_vectors' no está definida o está vacía. Esta celda requiere que el modelo esté cargado.\")"],"metadata":{"id":"4NL1TC3WSRa9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 8. (Opcional) Visualización de Embeddings\n","\n","Como los vectores tienen muchas dimensiones (ej: 300), no podemos graficarlos directamente. Pero podemos usar técnicas de **reducción de dimensionalidad** como PCA o (mejor para visualización) t-SNE para proyectarlos a 2D y ver si las palabras relacionadas se agrupan."],"metadata":{"id":"Gv35TEoHSmzY"}},{"cell_type":"code","source":["# --- Visualización Opcional ---\n","if word_vectors:\n","    # Lista de palabras para visualizar (elegir grupos relacionados)\n","    palabras_a_visualizar = [\n","        'perro', 'gato', 'caballo', 'pez', 'pájaro', # Animales\n","        'casa', 'apartamento', 'hogar', 'edificio', # Hogar\n","        'comer', 'beber', 'cocinar', 'correr', 'saltar', # Verbos\n","        'feliz', 'contento', 'triste', 'enojado', # Emociones\n","        'rey', 'reina', 'hombre', 'mujer' # Relación clásica\n","    ]\n","\n","    # Filtrar palabras que SÍ están en el vocabulario\n","    palabras_presentes = [p for p in palabras_a_visualizar if p in word_vectors]\n","    vectores_presentes = [word_vectors[p] for p in palabras_presentes]\n","\n","    if len(vectores_presentes) > 1: # Necesitamos al menos 2 puntos para visualizar\n","        print(f\"\\nVisualizando {len(palabras_presentes)} palabras...\")\n","\n","        # Reducción de dimensionalidad con t-SNE (suele dar mejores clusters visuales)\n","        tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(palabras_presentes)-1)) # Perplexity debe ser menor que n_samples\n","        vectores_2d = tsne.fit_transform(np.array(vectores_presentes))\n","\n","        # Crear el gráfico\n","        plt.figure(figsize=(12, 10))\n","        sns.scatterplot(x=vectores_2d[:, 0], y=vectores_2d[:, 1])\n","\n","        # Añadir etiquetas a los puntos\n","        for i, palabra in enumerate(palabras_presentes):\n","            plt.annotate(palabra, (vectores_2d[i, 0], vectores_2d[i, 1]), fontsize=9)\n","\n","        plt.title('Visualización de Word Embeddings (t-SNE)')\n","        plt.xlabel('Dimensión 1')\n","        plt.ylabel('Dimensión 2')\n","        plt.grid(True)\n","        plt.show()\n","    else:\n","        print(\"No hay suficientes palabras presentes en el vocabulario para visualizar.\")\n","\n","else:\n","    print(\"\\nNo se pudieron cargar los vectores. Saltando la visualización.\")"],"metadata":{"id":"gTyb2KT7R7dw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 9. Micro-Laboratorio (Ejercicio Práctico)\n","\n","**Consigna:** (Asumiendo que `word_vectors` se cargó correctamente)\n","\n","1.  **Exploración de Similitud:**\n","    *   Elegir 5 palabras que les interesen (intenten variar: un lugar, una profesión, un concepto abstracto, una comida, un sentimiento).\n","    *   Para cada una, usar `word_vectors.most_similar()` para encontrar las 5 palabras más parecidas.\n","    *   Anotar los resultados. ¿Les parecen lógicos? ¿Hay alguna similitud sorprendente o extraña?\n","\n","2.  **Prueba de Analogías:**\n","    *   Inventar y probar 3 analogías diferentes usando `word_vectors.most_similar(positive=[...], negative=[...])`.\n","    *   Ideas:\n","        *   `programador` es a `computadora` como `médico` es a `?`\n","        *   `Argentina` es a `peso` como `Japón` es a `?`\n","        *   `caminar` es a `pierna` como `hablar` es a `?`\n","    *   Verificar que todas las palabras de la analogía estén en el vocabulario antes de probarla.\n","    *   Anotar los resultados. ¿Funcionan las analogías como esperaban?\n","\n","3.  **(Opcional) Medir Similitud:**\n","    *   Elegir 3 pares de palabras:\n","        *   Un par de sinónimos claros (ej: `estudiante`, `alumno`).\n","        *   Un par de antónimos (ej: `grande`, `pequeño`).\n","        *   Un par de palabras no relacionadas (ej: `nube`, `zapato`).\n","    *   Calcular `word_vectors.similarity()` para cada par.\n","    *   ¿Los valores de similitud reflejan la relación entre las palabras? (Esperamos alta para sinónimos, baja/media-baja para no relacionadas, y ¿qué pasa con antónimos?).\n","\n","**¡Asegúrense de que las palabras que usan existan en `word_vectors`!**"],"metadata":{"id":"nC63WVgQSvzt"}},{"cell_type":"code","source":[],"metadata":{"id":"-ktJ7kTkR7XV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 10. Brainstorming: Sesgos en Embeddings\n","\n","Hemos visto que los embeddings capturan relaciones del lenguaje tal como aparecen en los datos de entrenamiento (el corpus masivo).\n","\n","**Pregunta clave:** Si esos datos contienen **sesgos sociales** (de género, raciales, de profesión, etc.), ¿qué creen que pasará con los embeddings?\n","\n","*   ¿Se reflejarán esos sesgos en las relaciones entre vectores? (Pista: ¡Sí!)\n","    *   Ejemplo famoso: `doctor - hombre + mujer = ?` a veces da `enfermera` en modelos entrenados en textos antiguos o sesgados. `programador - hombre + mujer = ?` podía dar `ama de casa`.\n","*   ¿Qué implicancias tiene esto si usamos estos embeddings para tareas como selección de personal, análisis de opiniones, o generación de texto?\n","*   Si el corpus no representa bien la diversidad lingüística (dialectos, jergas, lenguaje inclusivo), ¿cómo afectará eso a los embeddings de esas palabras (si es que existen)?\n","*   **¿Cómo podemos entrenar word embeddings que sean más inclusivos y representativos?** ¿Es posible \"limpiar\" o \"corregir\" (debias) los embeddings pre-entrenados? ¿Es nuestra responsabilidad como desarrolladores ser conscientes de esto y mitigarlo?\n","\n","**(Discusión en grupo)**"],"metadata":{"id":"VtyameEOTEXn"}}]}