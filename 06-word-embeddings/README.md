# MÃ³dulo 6: Word Embeddings

## ğŸ¯ Objetivos del MÃ³dulo

- Comprender los fundamentos de word embeddings
- Implementar Word2Vec (CBOW y Skip-gram)
- Explorar GloVe y FastText
- Trabajar con vectores preentrenados en espaÃ±ol

## ğŸ“š Contenido

### PrÃ¡ctica (PRA/)
- `004_GloVe_y_FastText.ipynb`: ImplementaciÃ³n de GloVe y FastText
- `005_Word_Embeddings_y_Word2Vec.ipynb`: Word2Vec desde cero
- **vectors/**: `SBW-vectors-300-min5.bin.gz` - Vectores preentrenados

### TeorÃ­a (TEO/)
- `001_Preprocesamiento_Avanzado.ipynb`: TÃ©cnicas avanzadas
- `002_RepresentaciÃ³n_de_Texto.ipynb`: Fundamentos teÃ³ricos
- `003_Anexo_Integrador.ipynb`: Ejercicios integradores

## ğŸ§  Conceptos Fundamentales

### Word2Vec
- **CBOW**: Predecir palabra central desde contexto
- **Skip-gram**: Predecir contexto desde palabra central
- Entrenamiento con negative sampling
- OptimizaciÃ³n con hierarchical softmax

### GloVe (Global Vectors)
- FactorizaciÃ³n de matriz de co-ocurrencias
- CombinaciÃ³n de mÃ©todos globales y locales
- Ventajas sobre Word2Vec tradicional

### FastText
- ExtensiÃ³n de Word2Vec con subpalabras
- Manejo de palabras fuera del vocabulario
- Especialmente Ãºtil para idiomas morfolÃ³gicamente ricos

## ğŸ”§ Herramientas Utilizadas

```python
import gensim
from gensim.models import Word2Vec, FastText
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
```

## ğŸ“ Competencias Desarrolladas

- âœ… ImplementaciÃ³n de algoritmos de embeddings
- âœ… Uso de vectores preentrenados
- âœ… EvaluaciÃ³n de calidad de embeddings
- âœ… VisualizaciÃ³n de espacios vectoriales

## ğŸ¯ MetodologÃ­a

- **Brainstorming**: "Â¿CÃ³mo podemos entrenar word embeddings que sean mÃ¡s inclusivos y representativos de la diversidad lingÃ¼Ã­stica?"
- **ExperimentaciÃ³n**: ComparaciÃ³n de diferentes algoritmos
- **AplicaciÃ³n**: Uso de vectores en espaÃ±ol argentino

---
*Prof. Matias Barreto - Laboratorio de IntroducciÃ³n al NLP y LLMs*
